## 3.5 DCU编程优化实战

​	本小节我们以GEMM程序为例，通过逐步对程序进行优化，阐述DCU编程优化过程中的一些常用的优化策略。

### 3.5.1 GEMM介绍

​	什么是GEMM？它的英文全称是GEneral Matrix Multiplication。GEMM运算广泛的应用于科学计算领域，比如：航空航天、天气预报、石油勘探、流体力学等。近些年来，随着深度学习的兴起以及深度学习模型和GEMM运算的高度相关性，GEMM运算及其优化变得更加重要。

​	GEMM运算的计算公式如下：

​                                               $C=alpha*op(A)*op(B) + beta*C$

其中，$alpha$和$beta$为标量因子，$op$为转置或者不转置的操作，$A、B、C$为矩阵。

​	假设$alpha=1.0,beta=1.0$，$op$为不转置的操作，则GEMM的计算公式可表示为：

​                                              $C+=A*B$

​	假设A:M行K列，B:K行N列，C:M行N列，则计算的伪代码如下：

```C++
for(int m = 0; m < M; m++)
{
    for(int n = 0; n < N; n++)
    {
        for(int k = 0; k < K; k++)
        {
            C[m][n] += A[m][k]*B[k][n];
        }
    }
}
```

可以看出，该伪代码计算操作总数为2MNK次（其中，M,N,K分别表示三层循环执行的次数，2表示循环最内层的一次乘累加操作）；内存访问操作总数为4MNK（其中，4表示对A矩阵，B矩阵以及C矩阵的内存访问，需要先读取内存，乘累加完成后存储，忽略对初始化时的操作）。

​	由上述代码可以看出，计算C矩阵的一个值，需要A矩阵的一行与B矩阵的一列元素做乘累加的操作，如图3.5.1.1所示。

![](https://s2.loli.net/2022/01/15/mBhEf2OK6atPzFA.png)

​	                                                              图3.5.1.1 C矩阵单个元素求解

​	如果计算C矩阵标记出元素的右边相邻的一个元素，则A矩阵相同行的元素会被重复读取一次；同样地，如果计算C矩阵标记处元素的下边相邻的一个元素，则B矩阵相同列的元素会被重复读取一次。由此可知，GEMM计算中存在大量的访存操作并且数据的重用性很低。

​	那么基于GEMM算法的这个伪代码，如何提高数据的重用性并减少访存，就是一个重要的优化策略。后续的优化方法均以此为基础。

### 3.5.2 GEMM优化中的分块策略

​	目前，在很多的GEMM优化方法中，会使用如下一种循环的方式进行计算：

```C++
for(int k = 0; k < K; k++)
{
    for(int m = 0; m < M; m++)
    {
        for(int n = 0; n < n; n++)
        {
            C[m][n] += A[m][k]*B[k][n];
        }
    }
}
```

在这种计算方法中，最外层循环每变化一次，会读取A矩阵的一列和B矩阵的一行，其中所有的元素两两相乘，得到C矩阵的一“层”，C矩阵的每个位置都会被写入一次。此时，这一行、一列元素就完成了自己的计算任务，就可以被扔掉了，计算如图3.5.2.1所示。

![](https://s2.loli.net/2022/01/15/4AgZ8l5eFQLzuGq.png)

​															 图3.5.2.1 C矩阵每“层”计算方式

​	假设矩阵的维度尺寸很小，我们按照此种方式，可以每次将A矩阵的一列和B矩阵的一行，读取到寄存器中，然后从寄存器中读取数据计算中C矩阵每个位置的部分值，避免每次从A矩阵、B矩阵对应的内存中，不断的访存，实现数据的复用，减少GEMM算法访存的开销。

​	如果矩阵的维度尺寸比较大，我们没有办法一次将A矩阵的一列和B矩阵的一行，读取到寄存器中，这个时候该如何处理呢？这就要引出GEMM优化中的分块了，我们可以在每个分块后的小矩阵中使用这种计算方式。分块的方式包含：1.以C矩阵为对象沿m、n方向的分块；2.以A矩阵、B矩阵为对象沿k方向的分块，分块方式如图3.5.2.2所示。

​	                   ![](https://s2.loli.net/2022/01/15/eEio3xuUNJp7FBW.png)

​															图3.5.2.2 GEMM计算分块方法

​		下面我们以m,n维度1x4、4x4的分块以及k维度depth=4的分块为基础，进行优化效果分析。

- 1x4的分块

  ​	以1x4的分块方式分块后，计算伪代码如下：

  ```C++
  for(int m = 0; m < M; m++)
  {
      for(int n = 0; n < N; n+=4)
      {
          C[m][n+0] = 0;
          C[m][n+1] = 0;
          C[m][n+2] = 0;
          C[m][n+3] = 0;
          for(int k = 0; k < K; k++)
          {
           	for(int subm = m; subm < m+1; subm++){
                  for(int subn = n; subn< n+4; subn++){
                       C[subm][subn] += A[subm][k]*B[k][subn];
                  }
              }   
          }
      }
  }
  ```

  对分块后，子块计算的伪代码进行循环展开，可得：

  ```C++
  for(int m = 0; m < M; m++)
  {
      for(int n = 0; n < N; n++)
      {
          C[m][n+0] = 0;
          C[m][n+1] = 0;
          C[m][n+2] = 0;
          C[m][n+3] = 0;
          for(int k = 0; k < K; k++)
          {
           	C[m][n+0] = A[m][k] * B[k][n+0];
              C[m][n+1] = A[m][k] * B[k][n+1];
              C[m][n+2] = A[m][k] * B[k][n+2];
              C[m][n+3] = A[m][k] * B[k][n+3];
          }
      }
  }
  ```

  1x4的分块方式，如图3.5.2.3所示。

  ![](https://s2.loli.net/2022/01/15/3G8RkcjLqNApJDW.png)

  ​															图3.5.2.3 1x4分块方法

  上图中将输出的计算矩阵拆分为1x4的小块，计算该块的输出时，需要使用A矩阵的1行和B矩阵的4列。从上述伪代码可以看出，这里的计算操作总数仍为2MNK，这一点在优化的过程中不会改变。这里的内存访问操作总数仍然是4MNK，但是通过观察代码我们发现，最内层的$A[m][k]$的元素是一致的，如果我们将其读取到寄存器中，可以实现4次数据的复用。此时，内存访问操作的次数变为(3+1/4)MNK。1/4是对A矩阵访存进行优化后，产生的效果。

- 4x4的分块

  以4x4的分块方式分块后，计算伪代码如下：

  ```C++
  for(int m = 0; m < M; m+=4)
  {
      for(int n = 0; n < N; n+=4)
      {
          C[m+0][n+0...3] = 0;
          C[m+1][n+0...3] = 0;
          C[m+2][n+0...3] = 0;
          C[m+3][n+0...3] = 0;
          for(int k = 0; k < K; k++)
          {
           	for(int subm = m; subm < m+4; subm++){
                  for(int subn = n; subn< n+4; subn++){
                       C[subm][subn] += A[subm][k]*B[k][subn];
                  }
              }   
          }
      }
  }
  ```

  对分块后，子块计算的伪代码进行循环展开，可得：

  ```C++
  for(int m = 0; m < M; m+=4)
  {
      for(int n = 0; n < N; n+=4)
      {
          C[m+0][n+0...3] = 0;
          C[m+1][n+0...3] = 0;
          C[m+2][n+0...3] = 0;
          C[m+3][n+0...3] = 0;
          for(int k = 0; k < K; k++)
          {
           	C[m+0][n+0...3] = A[m+0][k] * B[k][n+0...3];
              C[m+1][n+0...3] = A[m+1][k] * B[k][n+0...3];
              C[m+2][n+0...3] = A[m+2][k] * B[k][n+0...3];
              C[m+3][n+0...3] = A[m+3][k] * B[k][n+0...3];  
          }
      }
  }
  ```

  4x4的分块方式，如图3.5.2.4所示。

  ![](https://s2.loli.net/2022/01/15/gIXHxbUl9k4Punv.png)

  

  ​														图3.5.2.4 4x4分块方法

  上图中将输出的计算矩阵拆分为4x4的小块，计算该块的输出时，需要使用A矩阵的4行和B矩阵的4列。从上述伪代码可以看出，$A[m+0...3][k]、B[k][n+0...3]$均可以读取到寄存器中，进行4次复用。此时，内存访问操作的次数缩减为(2+1/4+1/4)MNK=(2+1/2)MNK，相对于最原始的4MNK次数的访存，有了1.6倍的减少。

- 4x4的分块+K维度depth=4的分块

  以4x4的分块与K维度depth为4的方式分块后，计算伪代码如下：

```C++
for(int m = 0; m < M; m+=4)
{
    for(int n = 0; n < N; n+=4)
    {
        C[m+0][n+0...3] = 0;
        C[m+1][n+0...3] = 0;
        C[m+2][n+0...3] = 0;
        C[m+3][n+0...3] = 0;
        for(int k = 0; k < K; k+=4)
        {
            for(int subk = k; subk < k+4; subk++){
                for(int subm = m; subm < m+4; subm++){
                    for(int subn = n; subn< n+4; subn++){
                         C[subm][subn] += A[subm][subk]*B[subk][subn];
                    }
            	}   
            }
        }
    }
}
```

对分块后，子块计算的伪代码进行循环展开，可得：

```C++
for(int m = 0; m < M; m+=4)
{
    for(int n = 0; n < N; n+=4)
    {
        C[m+0][n+0...3] = 0;
        C[m+1][n+0...3] = 0;
        C[m+2][n+0...3] = 0;
        C[m+3][n+0...3] = 0;
        for(int k = 0; k < K; k+=4)
        {
            C[m+0...3][n+0...3] = A[m+0...3][k+0] * B[k+0][n+0...3];
            C[m+0...3][n+0...3] = A[m+0...3][k+1] * B[k+1][n+0...3];
            C[m+0...3][n+0...3] = A[m+0...3][k+2] * B[k+2][n+0...3];
            C[m+0...3][n+0...3] = A[m+0...3][k+3] * B[k+3][n+0...3];  
        }
    }
}
```

这种分块方式，如图3.5.2.5所示。

![](https://s2.loli.net/2022/01/15/JH9d2kO6Cgqy8WP.png)

​															图3.5.2.5 4x4与K维度depth=4的分块方法

上图中将输出的计算矩阵C拆分为4x4的小块，计算该块的输出时，将K维度也进行了分块。在对M和N展开时，我们可以分别复用A矩阵 和B矩阵的数据；在对K展开时，我们可以将部分和累加在寄存器中，最内层循环一次迭代结束时一次写到C的内存中。那么内存访问的次数变为：MN + 1/4MNK + 1/4MNK ≈ 1/2MNK，相对于最原始版本有8倍的改进。

​	本小节以一个小例子，对分块和数据复用的优化策略以及达到的效果进行分析，后面我们在DCU上的优化以此为基础。

​	下面我们分不同的小节，以M=64，N=64，K=128，TN类型的GEMM为例，编写代码并采用不同的优化策略逐步进行优化。

### 3.5.3 GEMM基于DCU的初步实现

​	在我们的初步代码实现中，以C矩阵为对象，将其以4x4的小矩阵进行划分，如下：

![](https://s2.loli.net/2022/01/17/HOXusMiqJaevocK.png)

​															图3.5.2.6 C矩阵以4x4的小矩阵分块

由前面的M=64，N=64的条件，我们可以将C矩阵分为16x16个子块。在本小节，基于DCU的实现中，我们用每个线程负责一个4x4小块区域的计算。实现kernel代码以及CPU端的校验代码如下：

```C++
#include <iostream>
#include <stdio.h>
#include <string.h>
#include "hip/hip_runtime.h"

#include <sys/time.h>

#define NUM 256

#define WARMUP

struct my_timer
{
    struct timeval start_time, end_time;
    double time_use; // us
    
    void start(){
		gettimeofday(&start_time, NULL);
    }

    void stop(){
		gettimeofday(&end_time, NULL);
		time_use = (end_time.tv_sec-start_time.tv_sec)*1.0e6 + end_time.tv_usec-start_time.tv_usec;
    }	
};

#define HIP_CHECK(stat)                                                    \
{                                                                          \
	if(stat != hipSuccess)                                                 \
	{                                                                      \
		std::cerr << "Error: hip error in line " << __LINE__ << std::endl; \
		exit(-1);                                                          \
	}                                                                      \
}		

__global__ void three_cycle(double *src_a, double *src_b, double *dst_c, double alpha, double beta, int size_m, int size_n, int size_k)
{
	unsigned int serial  = threadIdx.x;     // 1D 线程号
	unsigned int index_j = (serial >> 4)*4;
	unsigned int index_i = (serial & 15)*4;
	
	unsigned int offset_c = index_i * size_n + index_j;
	unsigned int offset_a = index_i;
	unsigned int offset_b = index_j;
	
	double *tmp_c = dst_c + offset_c;
	double *tmp_a = src_a + offset_a;
	double *tmp_b = src_b + offset_b;
	
	int i,j,k;
	double a0,a1,a2,a3,b0,b1,b2,b3,c0;
	for(i=0; i<4; i++)
	{
		for(j=0; j<4; j++)
		{
			double sum = 0;
			for(k=0; k<size_k; k+=4)
			{
				a0 = *(tmp_a + k*size_m + i);       a1 = *(tmp_a + (k + 1)*size_m + i);
				a2 = *(tmp_a + (k + 2)*size_m + i); a3 = *(tmp_a + (k + 3)*size_m + i);
				
				b0 = *(tmp_b + k*size_n + j);       b1 = *(tmp_b + (k + 1)*size_n + j);
				b2 = *(tmp_b + (k + 2)*size_n +j);  b3 = *(tmp_b + (k + 3)*size_n + j);
				
				sum += a0 * b0 + a1 * b1 + a2 * b2 + a3 * b3;
			}
			c0 = *(tmp_c+j);
			sum = sum * alpha + beta * c0;
			*(tmp_c+j) = sum;
		}
		
		tmp_c = tmp_c + size_n;
	}
}

void mul_cpu(double *src_a, double *src_b, double *dst_c, double alpha, double beta, int size_m, int size_n, int size_k)
{
	int i, j, k;
	for(i=0; i<size_m; i++)
	{
		for(j=0; j<size_n; j++)
		{
			double sum=0;
			for(k=0; k<size_k; k++)
			{
				sum += src_a[k*size_m+i] * src_b[k*size_n+j];
			}
			dst_c[i*size_n+j] = alpha*sum + beta*dst_c[i*size_n+j];
		}
	}
}

int main()
{
	double *src_a;
	double *src_b;
	double *out_cpu;
	double *out_cpu2;
	double *a_device, *b_device, *c_device;
	double *out_gpu;
	double alpha = 2.0;
	double beta  = 3.0;
	int size_m   = 64;
	int size_n   = 64;
	int size_k   = 128;

	int error = 0;
	// apply for resources on cpu
	src_a   = (double *)malloc(size_m*size_k*sizeof(double));
	assert(src_a != NULL);
	src_b   = (double *)malloc(size_k*size_n*sizeof(double));
	assert(src_b != NULL);
	out_cpu = (double *)malloc(size_m*size_n*sizeof(double));
	assert(out_cpu != NULL);
	out_gpu = (double *)malloc(size_m*size_n*sizeof(double));
	assert(out_gpu != NULL);
	out_cpu2 = (double *)malloc(size_m*size_n*sizeof(double));
	assert(out_cpu2 != NULL);
	// apply for resources on dcu
	HIP_CHECK( hipMalloc((void**)&a_device, size_m*size_k*sizeof(double)) );
	HIP_CHECK( hipMalloc((void**)&b_device, size_k*size_n*sizeof(double)) );
	HIP_CHECK( hipMalloc((void**)&c_device, size_m*size_n*sizeof(double)) );
    
	// Initialize array
	for(int i=0; i<size_m*size_k; i++)
	{
		src_a[i] = rand()%128;
	}

	for(int i=0; i<size_k*size_n; i++)
	{
		src_b[i] = rand()%128;
	}
	
	for(int i=0; i<size_m*size_n; i++)
	{
		out_gpu[i] = out_cpu[i] = out_cpu2[i] = rand()%128;
	}

	HIP_CHECK( hipMemcpy(a_device, src_a,   size_m*size_k*sizeof(double), hipMemcpyHostToDevice) );
	HIP_CHECK( hipMemcpy(b_device, src_b,   size_n*size_k*sizeof(double), hipMemcpyHostToDevice) );
    HIP_CHECK( hipMemcpy(c_device, out_cpu, size_m*size_n*sizeof(double), hipMemcpyHostToDevice) );

#ifdef WARMUP
	for(int iter=0; iter<5; ++iter)
	{
		hipLaunchKernelGGL(three_cycle, dim3(1,1,1), dim3(256,1,1), 0, 0, a_device, b_device, c_device, alpha, beta, size_m, size_n, size_k);
		HIP_CHECK( hipMemcpy(c_device, out_cpu, size_m*size_n*sizeof(double), hipMemcpyHostToDevice) );
	}	
#endif
	// calcaulate GEMM on dcu and record costs time
	double sum_costs1 = 0.0;
	my_timer timer1;
	for(int iter=0; iter<10; ++iter)
	{
		HIP_CHECK( hipMemcpy(c_device, out_cpu, size_m*size_n*sizeof(double), hipMemcpyHostToDevice) );
		hipDeviceSynchronize();
		timer1.start();
		hipLaunchKernelGGL(three_cycle, dim3(1,1,1), dim3(256,1,1), 0, 0, a_device, b_device, c_device, alpha, beta, size_m, size_n, size_k);
		hipDeviceSynchronize();
		timer1.stop();
		sum_costs1 += timer1.time_use;
	}
	HIP_CHECK( hipMemcpy(out_gpu, c_device, size_m*size_n*sizeof(double), hipMemcpyDeviceToHost) );

    // calcaulate GEMM on cpu and record costs time
	double sum_costs2 = 0.0;
	my_timer timer2;
	for(int iter=0; iter<10; ++iter)
	{
		memcpy(out_cpu, out_cpu2, size_m*size_n*sizeof(double));
		timer2.start();
		mul_cpu(src_a, src_b, out_cpu, alpha, beta, size_m, size_n, size_k);
		timer2.stop();
		sum_costs2 += timer2.time_use;
	}

    // check the correctness of the results
	for(int i=0; i<size_m*size_n; i++)
	{
		if(fabs(out_gpu[i]-out_cpu[i]) > 1.0e-6)
		{
			error++;
			printf("%d,%lf,%lf\n",i,out_gpu[i],out_cpu[i]);
		}
	}
	
	if(error == 0)
	{
		printf("************************result is ok\n");
	}
		
	double run_time_cpu = (sum_costs2/10)/1000.0;
	printf("cpu time is %f (ms), GFlops is %f GFlops\n\n",run_time_cpu, (double)(2*size_m*size_n*size_k)/(run_time_cpu*1000000));

	double run_time_dcu = (sum_costs1/10)/1000.0;
	printf("dcu time is %f (ms), GFlops is %f GFlops\n\n",run_time_dcu, (double)(2*size_m*size_n*size_k)/(run_time_dcu*1000000));
	
	printf("finish\n");

    // release the resources
	free(src_a);
	free(src_b);
	free(out_cpu);
	free(out_cpu2);
	free(out_gpu);

	hipFree(a_device);
	hipFree(b_device);
	hipFree(c_device);

	return 0;
}
```

​	下面我们对该代码各个模块的功能进行介绍，后续小节可以复用该小节的代码，因此后面小节仅给出kernel函数。

- 计时函数代码

```C++
struct my_timer
{
    struct timeval start_time, end_time;
    double time_use; // us
    
    void start(){
		gettimeofday(&start_time, NULL);
    }

    void stop(){
		gettimeofday(&end_time, NULL);
		time_use = (end_time.tv_sec-start_time.tv_sec)*1.0e6 + end_time.tv_usec-start_time.tv_usec;
    }	
};
```

​	我们把该计时函数写成一个结构体，内部核心的函数是gettimeofday，该函数是C/C++中的一个计时函数，它的精度可以达到微秒级别。我们可以用类似下面的写法进行计时：

```C++
my_timer timer1;
timer1.start();
mul_cpu(src_a, src_b, out_cpu, alpha, beta, size_m, size_n, size_k);
timer1.stop();
double time_costs = timer1.use; // us
```

- HIP_CHECK宏

```C++
#define HIP_CHECK(stat)                                                    \
{                                                                          \
	if(stat != hipSuccess)                                                 \
	{                                                                      \
		std::cerr << "Error: hip error in line " << __LINE__ << std::endl; \
		exit(-1);                                                          \
	}                                                                      \
}	
```

该宏定义，主要是为了验证比如hipMalloc，hipMemcpy等一些设备端的函数的返回值是否正确，如果不正确，则程序不会往下运行，会直接退出。

- mul_cpu函数

```C++
void mul_cpu(double *src_a, double *src_b, double *dst_c, double alpha, double beta, int size_m, int size_n, int size_k)
{
	int i, j, k;
	for(i=0; i<size_m; i++)
	{
		for(j=0; j<size_n; j++)
		{
			double sum=0;
			for(k=0; k<size_k; k++)
			{
				sum += src_a[k*size_m+i] * src_b[k*size_n+j];
			}
			dst_c[i*size_n+j] = alpha*sum + beta*dst_c[i*size_n+j];
		}
	}
}
```

该函数是GEMM算法的串行实现，主要是为了验证DCU kernel计算结果是否正确。

- main函数

​    该函数中，包含CPU端和DCU端内存资源的分配、CPU端数组的初始化、将初始化的值copy到DCU端，然后用

hipLaunchKernelGGL进行kernel函数的调用并对执行时间进行统计。接着调用CPU端的GEMM函数mul_cpu，进行正确性的对比验证，最后计算CPU端和DCU端的GEMM的浮点计算性能。

- kernel函数-three_cycle

​    在该代码中，我们使用1个线程块，并且该线程块只有x维度不为1，线程块中有256个线程。每个线程，映射到对应的4x4的小矩阵块：

```C++
unsigned int serial  = threadIdx.x;     // 1D 线程号
unsigned int index_j = (serial >> 4)*4; // 当前小矩阵块左上角元素所在的C矩阵的列
unsigned int index_i = (serial & 15)*4; // 当前小矩阵块左上角元素所在的C矩阵的行
```

为了便于理解，当前线程的待处理的小矩阵块的左上角元素所在的C矩阵位置如下图所示。

<img src="https://s2.loli.net/2022/01/19/hmgxcVi7jD9fN6H.png" style="zoom:80%;" />

​																图3.5.2.7 当前线程处理的小矩阵块左上角的索引

其中，(serial >> 4)=serial/16，serial & 15 = serial%16。

接下来，计算出该小矩阵块区域GEMM求解，所需要的的A矩阵、B矩阵、C矩阵的起始偏移，代码如下：

```C++
unsigned int offset_c = index_i * size_n + index_j;
unsigned int offset_a = index_i;
unsigned int offset_b = index_j;

double *tmp_c = dst_c + offset_c;
double *tmp_a = src_a + offset_a;
double *tmp_b = src_b + offset_b;
```

当前线程处理的小矩阵块所需的矩阵A、矩阵B的区域如图3.5.2.8所示。

<img src="https://s2.loli.net/2022/01/19/faEKmuIgo8e73XT.png" style="zoom:80%;" />

​													图3.5.2.8 当前线程处理的小矩阵块所需的矩阵A、矩阵B的区域

接下来该kernel内部，有一个三层循环，用于计算C矩阵4x4小块内的值：

```C++
	int i,j,k;
	double a0,a1,a2,a3,b0,b1,b2,b3,c0;
	for(i=0;i<4;i++)
	{
		for(j=0;j<4;j++)
		{
			double sum = 0;
			for(k=0;k<size_k;k+=4)
			{
				a0 = *(tmp_a + k*size_m + i);a1 = *(tmp_a + (k + 1) * size_m + i);
				a2 = *(tmp_a + (k + 2)*size_m + i);a3 = *(tmp_a + (k + 3)*size_m + i);
				
				b0 = *(tmp_b + k*size_n + j);b1 = *(tmp_b + (k + 1)*size_n + j);
				b2 = *(tmp_b + (k + 2)*size_n +j);b3 = *(tmp_b + (k + 3)*size_n + j);
				
				sum += a0 * b0 + a1 * b1 + a2 * b2 + a3 * b3;
			}
			c0 = *(tmp_c+j);
			sum = sum * alpha + beta * c0;
			*(tmp_c+j) = sum;
		}
		
		tmp_c = tmp_c + size_n;
	}
```

该部分代码，在K方向，每次读取A矩阵的4个数据（按行）和B矩阵的4个数据（按列）,直到最内层循环结束为止。每次计算一个c值，计算完后再计算其他c值。

​	该代码初步实现了，GEMM算法到DCU上的移植，经测试浮点计算性能为2.54 GFlops。通过分析，我们会发现如同3.5.2中分析的那样，A矩阵、B矩阵中的数据存在大量的重复读取。我们可以从减少重复数据读取为优化点，进行进一步的优化。

### 3.5.4 利用寄存器实现数据复用

​	在本小节的优化中，我们从A矩阵读取的4个数据，B矩阵读取的4个数据以及中间结果全都存储到寄存器中，并且按照3.5.2中的策略，每次读取的A、B数据用来进行4x4小块内所有位置值的累加更新；接着沿K的方向以depth=8进行循环展开。由于3.5.3包含测试相关的代码，因此下面只列出kernel相关的代码：

```C++
#include <iostream>
#include <stdio.h>
#include <string.h>
#include "hip/hip_runtime.h"
#include <sys/time.h>

#define NUM 256

#define WARMUP

#define SCALAR_ZERO (double)(0)

/* MAC's */
#define MAC(A,B,DST) DST += A*B
#define TYPE_MAC(MULA,MULB,DST) DST = MAC(MULA,MULB,DST);
#define TT 4

/* 4x4 micro-tile */
#define MAC_4x4\
  TYPE_MAC(rA[0],rB[0],rC[0]); \
  TYPE_MAC(rA[0],rB[1],rC[1]); \
  TYPE_MAC(rA[0],rB[2],rC[2]); \
  TYPE_MAC(rA[0],rB[3],rC[3]); \
  TYPE_MAC(rA[1],rB[0],rC[4]); \
  TYPE_MAC(rA[1],rB[1],rC[5]); \
  TYPE_MAC(rA[1],rB[2],rC[6]); \
  TYPE_MAC(rA[1],rB[3],rC[7]); \
  TYPE_MAC(rA[2],rB[0],rC[8]); \
  TYPE_MAC(rA[2],rB[1],rC[9]); \
  TYPE_MAC(rA[2],rB[2],rC[10]); \
  TYPE_MAC(rA[2],rB[3],rC[11]); \
  TYPE_MAC(rA[3],rB[0],rC[12]); \
  TYPE_MAC(rA[3],rB[1],rC[13]); \
  TYPE_MAC(rA[3],rB[2],rC[14]); \
  TYPE_MAC(rA[3],rB[3],rC[15]); \
  
#define TYPE_MAC_WRITE(DST,SRC,ALPHA,REG,BETA) DST = 0 != (BETA) ? (ALPHA)*(REG) + (BETA)*(SRC) : (ALPHA)*(REG);

__global__ void global_depth8(double *src_a,double *src_b,double *dst_c, double alpha,double beta,int size_m,int size_n,int size_k)
{
	unsigned int serial  = threadIdx.x; //1D 线程号
	unsigned int index_j = (serial >> 4);
	unsigned int index_i = (serial & 15);
	
	unsigned int offset_c = (index_i * size_n + index_j) * 4;
	unsigned int offset_a = index_i * 4;
	unsigned int offset_b = index_j * 4;
	
	double *tmp_c = dst_c + offset_c;
	double *tmp_a = src_a + offset_a;
	double *tmp_b = src_b + offset_b;
	
	int i,j,k;
	
	double rA[4],rB[4],rC[16];
	
	rC[0] = SCALAR_ZERO;
        rC[1] = SCALAR_ZERO;
    	rC[2] = SCALAR_ZERO;
    	rC[3] = SCALAR_ZERO;
    	rC[4] = SCALAR_ZERO;
    	rC[5] = SCALAR_ZERO;
    	rC[6] = SCALAR_ZERO;
    	rC[7] = SCALAR_ZERO;
    	rC[8] = SCALAR_ZERO;
    	rC[9] = SCALAR_ZERO;
    	rC[10] = SCALAR_ZERO;
    	rC[11] = SCALAR_ZERO;
    	rC[12] = SCALAR_ZERO;
    	rC[13] = SCALAR_ZERO;
    	rC[14] = SCALAR_ZERO;
    	rC[15] = SCALAR_ZERO;
	
	for(i=0; i<size_k; i+=8)
	{
		//iter 0
		rA[0] = *(tmp_a + i*size_m + 0);
		rA[1] = *(tmp_a + i*size_m + 1);
		rA[2] = *(tmp_a + i*size_m + 2);
		rA[3] = *(tmp_a + i*size_m + 3);
		
		rB[0] = *(tmp_b + i*size_n + 0);
		rB[1] = *(tmp_b + i*size_n + 1);
		rB[2] = *(tmp_b + i*size_n + 2);
		rB[3] = *(tmp_b + i*size_n + 3);		
		MAC_4x4
		
		//iter 1
		rA[0] = *(tmp_a + (i+1)*size_m + 0);
		rA[1] = *(tmp_a + (i+1)*size_m + 1);
		rA[2] = *(tmp_a + (i+1)*size_m + 2);
		rA[3] = *(tmp_a + (i+1)*size_m + 3);
		
		rB[0] = *(tmp_b + (i+1)*size_n + 0);
		rB[1] = *(tmp_b + (i+1)*size_n + 1);
		rB[2] = *(tmp_b + (i+1)*size_n + 2);
		rB[3] = *(tmp_b + (i+1)*size_n + 3);		
		MAC_4x4	
		
		//iter 2
		rA[0] = *(tmp_a + (i+2)*size_m + 0);
		rA[1] = *(tmp_a + (i+2)*size_m + 1);
		rA[2] = *(tmp_a + (i+2)*size_m + 2);
		rA[3] = *(tmp_a + (i+2)*size_m + 3);
		
		rB[0] = *(tmp_b + (i+2)*size_n + 0);
		rB[1] = *(tmp_b + (i+2)*size_n + 1);
		rB[2] = *(tmp_b + (i+2)*size_n + 2);
		rB[3] = *(tmp_b + (i+2)*size_n + 3);		
		MAC_4x4	
		
		//iter 3
		rA[0] = *(tmp_a + (i+3)*size_m + 0);
		rA[1] = *(tmp_a + (i+3)*size_m + 1);
		rA[2] = *(tmp_a + (i+3)*size_m + 2);
		rA[3] = *(tmp_a + (i+3)*size_m + 3);
		
		rB[0] = *(tmp_b + (i+3)*size_n + 0);
		rB[1] = *(tmp_b + (i+3)*size_n + 1);
		rB[2] = *(tmp_b + (i+3)*size_n + 2);
		rB[3] = *(tmp_b + (i+3)*size_n + 3);		
		MAC_4x4	
		
		//iter 4
		rA[0] = *(tmp_a + (i+4)*size_m + 0);
		rA[1] = *(tmp_a + (i+4)*size_m + 1);
		rA[2] = *(tmp_a + (i+4)*size_m + 2);
		rA[3] = *(tmp_a + (i+4)*size_m + 3);
		
		rB[0] = *(tmp_b + (i+4)*size_n + 0);
		rB[1] = *(tmp_b + (i+4)*size_n + 1);
		rB[2] = *(tmp_b + (i+4)*size_n + 2);
		rB[3] = *(tmp_b + (i+4)*size_n + 3);		
		MAC_4x4	
		
		//iter 5
		rA[0] = *(tmp_a + (i+5)*size_m + 0);
		rA[1] = *(tmp_a + (i+5)*size_m + 1);
		rA[2] = *(tmp_a + (i+5)*size_m + 2);
		rA[3] = *(tmp_a + (i+5)*size_m + 3);
		
		rB[0] = *(tmp_b + (i+5)*size_n + 0);
		rB[1] = *(tmp_b + (i+5)*size_n + 1);
		rB[2] = *(tmp_b + (i+5)*size_n + 2);
		rB[3] = *(tmp_b + (i+5)*size_n + 3);		
		MAC_4x4	
		
		//iter 6
		rA[0] = *(tmp_a + (i+6)*size_m + 0);
		rA[1] = *(tmp_a + (i+6)*size_m + 1);
		rA[2] = *(tmp_a + (i+6)*size_m + 2);
		rA[3] = *(tmp_a + (i+6)*size_m + 3);
		
		rB[0] = *(tmp_b + (i+6)*size_n + 0);
		rB[1] = *(tmp_b + (i+6)*size_n + 1);
		rB[2] = *(tmp_b + (i+6)*size_n + 2);
		rB[3] = *(tmp_b + (i+6)*size_n + 3);		
		MAC_4x4	
		
		//iter 7
		rA[0] = *(tmp_a + (i+7)*size_m + 0);
		rA[1] = *(tmp_a + (i+7)*size_m + 1);
		rA[2] = *(tmp_a + (i+7)*size_m + 2);
		rA[3] = *(tmp_a + (i+7)*size_m + 3);
		
		rB[0] = *(tmp_b + (i+7)*size_n + 0);
		rB[1] = *(tmp_b + (i+7)*size_n + 1);
		rB[2] = *(tmp_b + (i+7)*size_n + 2);
		rB[3] = *(tmp_b + (i+7)*size_n + 3);		
		MAC_4x4	
		
	}
	
	TYPE_MAC_WRITE(*(tmp_c+0), *(tmp_c+0), alpha, rC[0], beta);
	TYPE_MAC_WRITE(*(tmp_c+1), *(tmp_c+1), alpha, rC[1], beta);
	TYPE_MAC_WRITE(*(tmp_c+2), *(tmp_c+2), alpha, rC[2], beta);
	TYPE_MAC_WRITE(*(tmp_c+3), *(tmp_c+3), alpha, rC[3], beta);
	
	TYPE_MAC_WRITE(*(tmp_c+0+size_n), *(tmp_c+0+size_n), alpha, rC[4], beta);
	TYPE_MAC_WRITE(*(tmp_c+1+size_n), *(tmp_c+1+size_n), alpha, rC[5], beta);
	TYPE_MAC_WRITE(*(tmp_c+2+size_n), *(tmp_c+2+size_n), alpha, rC[6], beta);
	TYPE_MAC_WRITE(*(tmp_c+3+size_n), *(tmp_c+3+size_n), alpha, rC[7], beta);
	
	TYPE_MAC_WRITE(*(tmp_c+0+size_n*2), *(tmp_c+0+size_n*2), alpha, rC[8], beta);
	TYPE_MAC_WRITE(*(tmp_c+1+size_n*2), *(tmp_c+1+size_n*2), alpha, rC[9], beta);
	TYPE_MAC_WRITE(*(tmp_c+2+size_n*2), *(tmp_c+2+size_n*2), alpha, rC[10], beta);
	TYPE_MAC_WRITE(*(tmp_c+3+size_n*2), *(tmp_c+3+size_n*2), alpha, rC[11], beta);
	
	TYPE_MAC_WRITE(*(tmp_c+0+size_n*3), *(tmp_c+0+size_n*3), alpha, rC[12], beta);
	TYPE_MAC_WRITE(*(tmp_c+1+size_n*3), *(tmp_c+1+size_n*3), alpha, rC[13], beta);
	TYPE_MAC_WRITE(*(tmp_c+2+size_n*3), *(tmp_c+2+size_n*3), alpha, rC[14], beta);
	TYPE_MAC_WRITE(*(tmp_c+3+size_n*3), *(tmp_c+3+size_n*3), alpha, rC[15], beta);	
}
```

4x4划块，depth=8 GEMM算法计算如下图所示：

<img src="https://s2.loli.net/2022/01/20/nKBrISk7Mat4mWc.png" style="zoom:80%;" />

​														图3.5.2.9 4x4划块，depth=8 GEMM算法示意图

下面我们对该代码各个模块的功能进行介绍。

- MAC_4x4宏

​    这个宏主要完成从A矩阵、B矩阵中读取的4个数据的乘累加操作，所有的操作均在寄存器中，每次计算会得到4x4最终结果的一“层”，直到沿K方向循环结束。	

```C++
/* MAC's */
#define MAC(A,B,DST) DST += A*B
#define TYPE_MAC(MULA,MULB,DST) DST = MAC(MULA,MULB,DST);

/* 4x4 micro-tile */
#define MAC_4x4\
  TYPE_MAC(rA[0],rB[0],rC[0]); \
  TYPE_MAC(rA[0],rB[1],rC[1]); \
  TYPE_MAC(rA[0],rB[2],rC[2]); \
  TYPE_MAC(rA[0],rB[3],rC[3]); \
  TYPE_MAC(rA[1],rB[0],rC[4]); \
  TYPE_MAC(rA[1],rB[1],rC[5]); \
  TYPE_MAC(rA[1],rB[2],rC[6]); \
  TYPE_MAC(rA[1],rB[3],rC[7]); \
  TYPE_MAC(rA[2],rB[0],rC[8]); \
  TYPE_MAC(rA[2],rB[1],rC[9]); \
  TYPE_MAC(rA[2],rB[2],rC[10]); \
  TYPE_MAC(rA[2],rB[3],rC[11]); \
  TYPE_MAC(rA[3],rB[0],rC[12]); \
  TYPE_MAC(rA[3],rB[1],rC[13]); \
  TYPE_MAC(rA[3],rB[2],rC[14]); \
  TYPE_MAC(rA[3],rB[3],rC[15]); \
```

该宏计算如图3.5.2.10所示。

![](https://s2.loli.net/2022/01/20/R75ok1ybvcdSjsX.png)

​															图3.5.2.10 MAC_4x4计算

也即是：c0=a0xb0, c1=a0xb1, c2=a0xb2, c3=a0xb3，...,  c15=a3xb3。每次沿图3.5.2.10红色箭头的方向，读取A矩阵4个数据，B矩阵4个数据。计算出4x4小矩阵块的一次部分值（可以看做一“层”），然后进行累加。

- 主体for循环

```C++
for(i=0; i<size_k; i+=8)
{
    //iter 0
    rA[0] = *(tmp_a + i*size_m + 0);
    rA[1] = *(tmp_a + i*size_m + 1);
    rA[2] = *(tmp_a + i*size_m + 2);
    rA[3] = *(tmp_a + i*size_m + 3);

    rB[0] = *(tmp_b + i*size_n + 0);
    rB[1] = *(tmp_b + i*size_n + 1);
    rB[2] = *(tmp_b + i*size_n + 2);
    rB[3] = *(tmp_b + i*size_n + 3);		
    MAC_4x4

     //iter 1
    rA[0] = *(tmp_a + (i+1)*size_m + 0);
    rA[1] = *(tmp_a + (i+1)*size_m + 1);
    rA[2] = *(tmp_a + (i+1)*size_m + 2);
    rA[3] = *(tmp_a + (i+1)*size_m + 3);

    rB[0] = *(tmp_b + (i+1)*size_n + 0);
    rB[1] = *(tmp_b + (i+1)*size_n + 1);
    rB[2] = *(tmp_b + (i+1)*size_n + 2);
    rB[3] = *(tmp_b + (i+1)*size_n + 3);		
    MAC_4x4	
        ...
}
```

​	通过代码分析可以看到，沿K维度方向，代码设置为depth=8并进行循环展开，循环展开能够增大指令调度的空间，减少循环分支指令的开销，即增大指令流水线的效率。同时，循环展开可以更好地实现数据预取技术。

​	对于每个4x4小块的计算，A矩阵和B矩阵的访存由16K次减少到4K次，减少了3/4。

- TYPE_MAC_WRITE

```C++
#define TYPE_MAC_WRITE(DST,SRC,ALPHA,REG,BETA) DST = 0 != (BETA) ? (ALPHA)*(REG) + (BETA)*(SRC) : (ALPHA)*(REG);
```

​	TYPE_MAC_WRITE宏的功能是，做最后的乘累加以及将寄存器中的数据写回到全局内存。

​	该代码利用寄存器实现了数据复用，减少了访存，经测试浮点计算性能为8.33 GFlops。在该代码实现中，数据读取完后，直接使用该数据进行计算，会存在计算指令等待访存完成的现象，直至读取操作完成，这会产生一定的延迟。我们可以使用数据预取技术将这种延迟隐藏掉。

### 3.5.5 利用数据预取隐藏延迟

​	在本小节的优化中，我们将读取A矩阵、B矩阵数据所用的寄存器的大小都扩大了一倍。第一组数据在循环外面进行读取，读取完后，没有直接进行计算，中间隔了下一组数据的读取，再进行乘累加计算。代码如下：

```C++
#include <iostream>
#include <stdio.h>
#include <string.h>
#include "hip/hip_runtime.h"
#include <sys/time.h>

#define NUM 256

#define WARMUP

#define SCALAR_ZERO (double)(0)

/* MAC's */
#define MAC(A,B,DST) DST += A*B
#define TYPE_MAC(MULA,MULB,DST) DST = MAC(MULA,MULB,DST);
#define TT 4

/* 4x4 micro-tile */
#define MAC_4x4\
  TYPE_MAC(rA[0],rB[0],rC[0]); \
  TYPE_MAC(rA[0],rB[1],rC[1]); \
  TYPE_MAC(rA[0],rB[2],rC[2]); \
  TYPE_MAC(rA[0],rB[3],rC[3]); \
  TYPE_MAC(rA[1],rB[0],rC[4]); \
  TYPE_MAC(rA[1],rB[1],rC[5]); \
  TYPE_MAC(rA[1],rB[2],rC[6]); \
  TYPE_MAC(rA[1],rB[3],rC[7]); \
  TYPE_MAC(rA[2],rB[0],rC[8]); \
  TYPE_MAC(rA[2],rB[1],rC[9]); \
  TYPE_MAC(rA[2],rB[2],rC[10]); \
  TYPE_MAC(rA[2],rB[3],rC[11]); \
  TYPE_MAC(rA[3],rB[0],rC[12]); \
  TYPE_MAC(rA[3],rB[1],rC[13]); \
  TYPE_MAC(rA[3],rB[2],rC[14]); \
  TYPE_MAC(rA[3],rB[3],rC[15]); \
  
#define MAC_4x4_BLK\
  TYPE_MAC(rA[0+TT],rB[0+TT],rC[0]); \
  TYPE_MAC(rA[0+TT],rB[1+TT],rC[1]); \
  TYPE_MAC(rA[0+TT],rB[2+TT],rC[2]); \
  TYPE_MAC(rA[0+TT],rB[3+TT],rC[3]); \
  TYPE_MAC(rA[1+TT],rB[0+TT],rC[4]); \
  TYPE_MAC(rA[1+TT],rB[1+TT],rC[5]); \
  TYPE_MAC(rA[1+TT],rB[2+TT],rC[6]); \
  TYPE_MAC(rA[1+TT],rB[3+TT],rC[7]); \
  TYPE_MAC(rA[2+TT],rB[0+TT],rC[8]); \
  TYPE_MAC(rA[2+TT],rB[1+TT],rC[9]); \
  TYPE_MAC(rA[2+TT],rB[2+TT],rC[10]); \
  TYPE_MAC(rA[2+TT],rB[3+TT],rC[11]); \
  TYPE_MAC(rA[3+TT],rB[0+TT],rC[12]); \
  TYPE_MAC(rA[3+TT],rB[1+TT],rC[13]); \
  TYPE_MAC(rA[3+TT],rB[2+TT],rC[14]); \
  TYPE_MAC(rA[3+TT],rB[3+TT],rC[15]); \

#define TYPE_MAC_WRITE(DST,SRC,ALPHA,REG,BETA) DST = 0 != (BETA) ? (ALPHA)*(REG) + (BETA)*(SRC) : (ALPHA)*(REG);
__global__ void global_depth8_preload_delay(double *src_a,double *src_b,double *dst_c, double alpha,double beta,int size_m,int size_n,int size_k)
{
	unsigned int serial  = threadIdx.x;   // 1D线程号
	unsigned int index_j = (serial >> 4);
	unsigned int index_i = (serial & 15);
	
	unsigned int offset_c = (index_i * size_n + index_j) * 4;
	unsigned int offset_a = index_i * 4;
	unsigned int offset_b = index_j * 4;
	
	double *tmp_c = dst_c + offset_c;
	double *tmp_a = src_a + offset_a;
	double *tmp_b = src_b + offset_b;
	
	int i,j,k;
	
	double rA[8],rB[8],rC[16];
	
	rC[0] = SCALAR_ZERO;
    rC[1] = SCALAR_ZERO;
    rC[2] = SCALAR_ZERO;
    rC[3] = SCALAR_ZERO;
    rC[4] = SCALAR_ZERO;
    rC[5] = SCALAR_ZERO;
    rC[6] = SCALAR_ZERO;
    rC[7] = SCALAR_ZERO;
    rC[8] = SCALAR_ZERO;
    rC[9] = SCALAR_ZERO;
    rC[10] = SCALAR_ZERO;
    rC[11] = SCALAR_ZERO;
    rC[12] = SCALAR_ZERO;
    rC[13] = SCALAR_ZERO;
    rC[14] = SCALAR_ZERO;
    rC[15] = SCALAR_ZERO;
		
	//preload
	rA[0] = *(tmp_a + 0);
	rA[1] = *(tmp_a + 1);
	rA[2] = *(tmp_a + 2);
	rA[3] = *(tmp_a + 3);
		
	rB[0] = *(tmp_b + 0);
	rB[1] = *(tmp_b + 1);
	rB[2] = *(tmp_b + 2);
	rB[3] = *(tmp_b + 3);
	
	
	for(i=1;i<size_k-8;i+=8)
	{
		//iter 0
		rA[0+TT] = *(tmp_a + i*size_m + 0);
		rA[1+TT] = *(tmp_a + i*size_m + 1);
		rA[2+TT] = *(tmp_a + i*size_m + 2);
		rA[3+TT] = *(tmp_a + i*size_m + 3);
		
		rB[0+TT] = *(tmp_b + i*size_n + 0);
		rB[1+TT] = *(tmp_b + i*size_n + 1);
		rB[2+TT] = *(tmp_b + i*size_n + 2);
		rB[3+TT] = *(tmp_b + i*size_n + 3);		
		MAC_4x4
		
		//iter 1
		rA[0] = *(tmp_a + (i+1)*size_m + 0);
		rA[1] = *(tmp_a + (i+1)*size_m + 1);
		rA[2] = *(tmp_a + (i+1)*size_m + 2);
		rA[3] = *(tmp_a + (i+1)*size_m + 3);
		
		rB[0] = *(tmp_b + (i+1)*size_n + 0);
		rB[1] = *(tmp_b + (i+1)*size_n + 1);
		rB[2] = *(tmp_b + (i+1)*size_n + 2);
		rB[3] = *(tmp_b + (i+1)*size_n + 3);		
		MAC_4x4_BLK	
		
		//iter 2
		rA[0+TT] = *(tmp_a + (i+2)*size_m + 0);
		rA[1+TT] = *(tmp_a + (i+2)*size_m + 1);
		rA[2+TT] = *(tmp_a + (i+2)*size_m + 2);
		rA[3+TT] = *(tmp_a + (i+2)*size_m + 3);
		
		rB[0+TT] = *(tmp_b + (i+2)*size_n + 0);
		rB[1+TT] = *(tmp_b + (i+2)*size_n + 1);
		rB[2+TT] = *(tmp_b + (i+2)*size_n + 2);
		rB[3+TT] = *(tmp_b + (i+2)*size_n + 3);		
		MAC_4x4	
		
		//iter 3
		rA[0] = *(tmp_a + (i+3)*size_m + 0);
		rA[1] = *(tmp_a + (i+3)*size_m + 1);
		rA[2] = *(tmp_a + (i+3)*size_m + 2);
		rA[3] = *(tmp_a + (i+3)*size_m + 3);
		
		rB[0] = *(tmp_b + (i+3)*size_n + 0);
		rB[1] = *(tmp_b + (i+3)*size_n + 1);
		rB[2] = *(tmp_b + (i+3)*size_n + 2);
		rB[3] = *(tmp_b + (i+3)*size_n + 3);		
		MAC_4x4_BLK	
		
		//iter 4
		rA[0+TT] = *(tmp_a + (i+4)*size_m + 0);
		rA[1+TT] = *(tmp_a + (i+4)*size_m + 1);
		rA[2+TT] = *(tmp_a + (i+4)*size_m + 2);
		rA[3+TT] = *(tmp_a + (i+4)*size_m + 3);
		
		rB[0+TT] = *(tmp_b + (i+4)*size_n + 0);
		rB[1+TT] = *(tmp_b + (i+4)*size_n + 1);
		rB[2+TT] = *(tmp_b + (i+4)*size_n + 2);
		rB[3+TT] = *(tmp_b + (i+4)*size_n + 3);		
		MAC_4x4	
		
		//iter 5
		rA[0] = *(tmp_a + (i+5)*size_m + 0);
		rA[1] = *(tmp_a + (i+5)*size_m + 1);
		rA[2] = *(tmp_a + (i+5)*size_m + 2);
		rA[3] = *(tmp_a + (i+5)*size_m + 3);
		
		rB[0] = *(tmp_b + (i+5)*size_n + 0);
		rB[1] = *(tmp_b + (i+5)*size_n + 1);
		rB[2] = *(tmp_b + (i+5)*size_n + 2);
		rB[3] = *(tmp_b + (i+5)*size_n + 3);		
		MAC_4x4_BLK	
		
		//iter 6
		rA[0+TT] = *(tmp_a + (i+6)*size_m + 0);
		rA[1+TT] = *(tmp_a + (i+6)*size_m + 1);
		rA[2+TT] = *(tmp_a + (i+6)*size_m + 2);
		rA[3+TT] = *(tmp_a + (i+6)*size_m + 3);
		
		rB[0+TT] = *(tmp_b + (i+6)*size_n + 0);
		rB[1+TT] = *(tmp_b + (i+6)*size_n + 1);
		rB[2+TT] = *(tmp_b + (i+6)*size_n + 2);
		rB[3+TT] = *(tmp_b + (i+6)*size_n + 3);		
		MAC_4x4	
		
		//iter 7
		rA[0] = *(tmp_a + (i+7)*size_m + 0);
		rA[1] = *(tmp_a + (i+7)*size_m + 1);
		rA[2] = *(tmp_a + (i+7)*size_m + 2);
		rA[3] = *(tmp_a + (i+7)*size_m + 3);
		
		rB[0] = *(tmp_b + (i+7)*size_n + 0);
		rB[1] = *(tmp_b + (i+7)*size_n + 1);
		rB[2] = *(tmp_b + (i+7)*size_n + 2);
		rB[3] = *(tmp_b + (i+7)*size_n + 3);		
		MAC_4x4_BLK		
	}
	
	//last loop
		//iter 0
		rA[0+TT] = *(tmp_a + i*size_m + 0);
		rA[1+TT] = *(tmp_a + i*size_m + 1);
		rA[2+TT] = *(tmp_a + i*size_m + 2);
		rA[3+TT] = *(tmp_a + i*size_m + 3);
		
		rB[0+TT] = *(tmp_b + i*size_n + 0);
		rB[1+TT] = *(tmp_b + i*size_n + 1);
		rB[2+TT] = *(tmp_b + i*size_n + 2);
		rB[3+TT] = *(tmp_b + i*size_n + 3);		
		MAC_4x4
		
		//iter 1
		rA[0] = *(tmp_a + (i+1)*size_m + 0);
		rA[1] = *(tmp_a + (i+1)*size_m + 1);
		rA[2] = *(tmp_a + (i+1)*size_m + 2);
		rA[3] = *(tmp_a + (i+1)*size_m + 3);
		
		rB[0] = *(tmp_b + (i+1)*size_n + 0);
		rB[1] = *(tmp_b + (i+1)*size_n + 1);
		rB[2] = *(tmp_b + (i+1)*size_n + 2);
		rB[3] = *(tmp_b + (i+1)*size_n + 3);		
		MAC_4x4_BLK	
		
		//iter 2
		rA[0+TT] = *(tmp_a + (i+2)*size_m + 0);
		rA[1+TT] = *(tmp_a + (i+2)*size_m + 1);
		rA[2+TT] = *(tmp_a + (i+2)*size_m + 2);
		rA[3+TT] = *(tmp_a + (i+2)*size_m + 3);
		
		rB[0+TT] = *(tmp_b + (i+2)*size_n + 0);
		rB[1+TT] = *(tmp_b + (i+2)*size_n + 1);
		rB[2+TT] = *(tmp_b + (i+2)*size_n + 2);
		rB[3+TT] = *(tmp_b + (i+2)*size_n + 3);		
		MAC_4x4	
		
		//iter 3
		rA[0] = *(tmp_a + (i+3)*size_m + 0);
		rA[1] = *(tmp_a + (i+3)*size_m + 1);
		rA[2] = *(tmp_a + (i+3)*size_m + 2);
		rA[3] = *(tmp_a + (i+3)*size_m + 3);
		
		rB[0] = *(tmp_b + (i+3)*size_n + 0);
		rB[1] = *(tmp_b + (i+3)*size_n + 1);
		rB[2] = *(tmp_b + (i+3)*size_n + 2);
		rB[3] = *(tmp_b + (i+3)*size_n + 3);		
		MAC_4x4_BLK	
		
		//iter 4
		rA[0+TT] = *(tmp_a + (i+4)*size_m + 0);
		rA[1+TT] = *(tmp_a + (i+4)*size_m + 1);
		rA[2+TT] = *(tmp_a + (i+4)*size_m + 2);
		rA[3+TT] = *(tmp_a + (i+4)*size_m + 3);
		
		rB[0+TT] = *(tmp_b + (i+4)*size_n + 0);
		rB[1+TT] = *(tmp_b + (i+4)*size_n + 1);
		rB[2+TT] = *(tmp_b + (i+4)*size_n + 2);
		rB[3+TT] = *(tmp_b + (i+4)*size_n + 3);		
		MAC_4x4	
		
		//iter 5
		rA[0] = *(tmp_a + (i+5)*size_m + 0);
		rA[1] = *(tmp_a + (i+5)*size_m + 1);
		rA[2] = *(tmp_a + (i+5)*size_m + 2);
		rA[3] = *(tmp_a + (i+5)*size_m + 3);
		
		rB[0] = *(tmp_b + (i+5)*size_n + 0);
		rB[1] = *(tmp_b + (i+5)*size_n + 1);
		rB[2] = *(tmp_b + (i+5)*size_n + 2);
		rB[3] = *(tmp_b + (i+5)*size_n + 3);		
		MAC_4x4_BLK	
		
		//iter 6
		rA[0+TT] = *(tmp_a + (i+6)*size_m + 0);
		rA[1+TT] = *(tmp_a + (i+6)*size_m + 1);
		rA[2+TT] = *(tmp_a + (i+6)*size_m + 2);
		rA[3+TT] = *(tmp_a + (i+6)*size_m + 3);
		
		rB[0+TT] = *(tmp_b + (i+6)*size_n + 0);
		rB[1+TT] = *(tmp_b + (i+6)*size_n + 1);
		rB[2+TT] = *(tmp_b + (i+6)*size_n + 2);
		rB[3+TT] = *(tmp_b + (i+6)*size_n + 3);		
		MAC_4x4	
				
		MAC_4x4_BLK	
	
	TYPE_MAC_WRITE(*(tmp_c+0), *(tmp_c+0), alpha, rC[0], beta);
	TYPE_MAC_WRITE(*(tmp_c+1), *(tmp_c+1), alpha, rC[1], beta);
	TYPE_MAC_WRITE(*(tmp_c+2), *(tmp_c+2), alpha, rC[2], beta);
	TYPE_MAC_WRITE(*(tmp_c+3), *(tmp_c+3), alpha, rC[3], beta);
	
	TYPE_MAC_WRITE(*(tmp_c+0+size_n), *(tmp_c+0+size_n), alpha, rC[4], beta);
	TYPE_MAC_WRITE(*(tmp_c+1+size_n), *(tmp_c+1+size_n), alpha, rC[5], beta);
	TYPE_MAC_WRITE(*(tmp_c+2+size_n), *(tmp_c+2+size_n), alpha, rC[6], beta);
	TYPE_MAC_WRITE(*(tmp_c+3+size_n), *(tmp_c+3+size_n), alpha, rC[7], beta);
	
	TYPE_MAC_WRITE(*(tmp_c+0+size_n*2), *(tmp_c+0+size_n*2), alpha, rC[8], beta);
	TYPE_MAC_WRITE(*(tmp_c+1+size_n*2), *(tmp_c+1+size_n*2), alpha, rC[9], beta);
	TYPE_MAC_WRITE(*(tmp_c+2+size_n*2), *(tmp_c+2+size_n*2), alpha, rC[10], beta);
	TYPE_MAC_WRITE(*(tmp_c+3+size_n*2), *(tmp_c+3+size_n*2), alpha, rC[11], beta);
	
	TYPE_MAC_WRITE(*(tmp_c+0+size_n*3), *(tmp_c+0+size_n*3), alpha, rC[12], beta);
	TYPE_MAC_WRITE(*(tmp_c+1+size_n*3), *(tmp_c+1+size_n*3), alpha, rC[13], beta);
	TYPE_MAC_WRITE(*(tmp_c+2+size_n*3), *(tmp_c+2+size_n*3), alpha, rC[14], beta);
	TYPE_MAC_WRITE(*(tmp_c+3+size_n*3), *(tmp_c+3+size_n*3), alpha, rC[15], beta);	
}
```

​	该部分代码，与3.5.5节代码的区别在于，从global memory中读取数据的方式。

```C++
	//preload
	rA[0] = *(tmp_a + 0);
	rA[1] = *(tmp_a + 1);
	rA[2] = *(tmp_a + 2);
	rA[3] = *(tmp_a + 3);
		
	rB[0] = *(tmp_b + 0);
	rB[1] = *(tmp_b + 1);
	rB[2] = *(tmp_b + 2);
	rB[3] = *(tmp_b + 3);
	
	for(i=1;i<size_k-8;i+=8)
	{
		//iter 0
		rA[0+TT] = *(tmp_a + i*size_m + 0);
		rA[1+TT] = *(tmp_a + i*size_m + 1);
		rA[2+TT] = *(tmp_a + i*size_m + 2);
		rA[3+TT] = *(tmp_a + i*size_m + 3);
		
		rB[0+TT] = *(tmp_b + i*size_n + 0);
		rB[1+TT] = *(tmp_b + i*size_n + 1);
		rB[2+TT] = *(tmp_b + i*size_n + 2);
		rB[3+TT] = *(tmp_b + i*size_n + 3);		
		MAC_4x4
		
		//iter 1
		rA[0] = *(tmp_a + (i+1)*size_m + 0);
		rA[1] = *(tmp_a + (i+1)*size_m + 1);
		rA[2] = *(tmp_a + (i+1)*size_m + 2);
		rA[3] = *(tmp_a + (i+1)*size_m + 3);
		
		rB[0] = *(tmp_b + (i+1)*size_n + 0);
		rB[1] = *(tmp_b + (i+1)*size_n + 1);
		rB[2] = *(tmp_b + (i+1)*size_n + 2);
		rB[3] = *(tmp_b + (i+1)*size_n + 3);		
		MAC_4x4_BLK	
		...
	}
```

在循环开始前，首先进行一次数据的预读取，放到rA[0...3], rB[0...3]中，然后进入循环后，将下一次用到的数据进行读取，放到rA[4...7], rB[4...7]中，接着执行MAC_4x4或者MAC_4x4_BLK。最后一个depth=8，放到循环外执行。

这样的操作，可以使预取下一次计算所需要的的数据的指令和本次计算的过程指令进行交叠，减少计算指令等待访存完成的现象，可以将访存的延迟给隐藏掉一部分，经测试，修改后的GEMM算法浮点计算性能为13.85GFlops。

​	在实际不同的应用中，采用数据预取时，会存在寄存器使用得比较多，导致性能下降的情况。在此种情况下，我们可以利用DCU中的共享内存LDS，提高访存的效率。

### 3.5.6 利用LDS搬移数据

​	本小节的代码，我们将使用LDS进行数据的搬移，在depth=8的情况下，每次将A矩阵4xdepth、B矩阵4xdepth个数据读取到LDS中，同步后，直接将数据从LDS中读取到寄存器中进行运算。代码如下：

```C++
#include <iostream>
#include <stdio.h>
#include <string.h>
#include "hip/hip_runtime.h"
#include <sys/time.h>

#define NUM 256

#define WARMUP

#define SCALAR_ZERO (double)(0)
#define LDS_NUM_ELEMENTS 2048
#define LDS_OFFSET_B 512
#define LDS_OFFSET_BLK 1024
#define DEPTH 8
#define MT0I 64
#define MT1J 64

/* MAC's */
#define MAC(A,B,DST) DST += A*B
#define TYPE_MAC(MULA,MULB,DST) DST = MAC(MULA,MULB,DST);
#define TT 4

/* 4x4 micro-tile */
#define MAC_4x4\
  TYPE_MAC(rA[0],rB[0],rC[0]); \
  TYPE_MAC(rA[0],rB[1],rC[1]); \
  TYPE_MAC(rA[0],rB[2],rC[2]); \
  TYPE_MAC(rA[0],rB[3],rC[3]); \
  TYPE_MAC(rA[1],rB[0],rC[4]); \
  TYPE_MAC(rA[1],rB[1],rC[5]); \
  TYPE_MAC(rA[1],rB[2],rC[6]); \
  TYPE_MAC(rA[1],rB[3],rC[7]); \
  TYPE_MAC(rA[2],rB[0],rC[8]); \
  TYPE_MAC(rA[2],rB[1],rC[9]); \
  TYPE_MAC(rA[2],rB[2],rC[10]); \
  TYPE_MAC(rA[2],rB[3],rC[11]); \
  TYPE_MAC(rA[3],rB[0],rC[12]); \
  TYPE_MAC(rA[3],rB[1],rC[13]); \
  TYPE_MAC(rA[3],rB[2],rC[14]); \
  TYPE_MAC(rA[3],rB[3],rC[15]); \
  
#define MAC_4x4_BLK\
  TYPE_MAC(rA[0+TT],rB[0+TT],rC[0]); \
  TYPE_MAC(rA[0+TT],rB[1+TT],rC[1]); \
  TYPE_MAC(rA[0+TT],rB[2+TT],rC[2]); \
  TYPE_MAC(rA[0+TT],rB[3+TT],rC[3]); \
  TYPE_MAC(rA[1+TT],rB[0+TT],rC[4]); \
  TYPE_MAC(rA[1+TT],rB[1+TT],rC[5]); \
  TYPE_MAC(rA[1+TT],rB[2+TT],rC[6]); \
  TYPE_MAC(rA[1+TT],rB[3+TT],rC[7]); \
  TYPE_MAC(rA[2+TT],rB[0+TT],rC[8]); \
  TYPE_MAC(rA[2+TT],rB[1+TT],rC[9]); \
  TYPE_MAC(rA[2+TT],rB[2+TT],rC[10]); \
  TYPE_MAC(rA[2+TT],rB[3+TT],rC[11]); \
  TYPE_MAC(rA[3+TT],rB[0+TT],rC[12]); \
  TYPE_MAC(rA[3+TT],rB[1+TT],rC[13]); \
  TYPE_MAC(rA[3+TT],rB[2+TT],rC[14]); \
  TYPE_MAC(rA[3+TT],rB[3+TT],rC[15]); \

#define TYPE_MAC_WRITE(DST,SRC,ALPHA,REG,BETA) DST = 0 != (BETA) ? (ALPHA)*(REG) + (BETA)*(SRC) : (ALPHA)*(REG)

__global__ void global_depth8_lds(double *src_a,double *src_b,double *dst_c, double alpha,double beta,int size_m,int size_n,int size_k)
{
	/* allocate local memory */
    __shared__ double localMemory[LDS_NUM_ELEMENTS];
	
	unsigned int serial = threadIdx.x;//线程号
	unsigned int grj = (serial >> 5);
	unsigned int gri = (serial & 31);
	
	unsigned int goa = grj * size_m + gri * 2;
	unsigned int gob = grj * size_n + gri * 2;
	
	double *local_write_A =  localMemory + serial * 2;
	double *local_write_B =  localMemory + serial * 2 + LDS_OFFSET_B;
	
	unsigned int lrj = (serial >> 4);
	unsigned int lri = (serial & 15);
	
	double *local_read_A = localMemory + lri * 4;
	double *local_read_B = localMemory + lrj * 4 + LDS_OFFSET_B;
	
	unsigned int goc = (lri * size_n + lrj ) * 4;
	
	double *global_address_C = dst_c + goc;
	double *global_address_A = src_a + goa;
	double *global_address_B = src_b + gob;
	
	int i,j,k;
	
	double rA[8],rB[8],rC[16];
	double global_a0,global_a1,global_b0,global_b1;
	
	rC[0] = SCALAR_ZERO;
        rC[1] = SCALAR_ZERO;
    	rC[2] = SCALAR_ZERO;
    	rC[3] = SCALAR_ZERO;
    	rC[4] = SCALAR_ZERO;
    	rC[5] = SCALAR_ZERO;
    	rC[6] = SCALAR_ZERO;
    	rC[7] = SCALAR_ZERO;
    	rC[8] = SCALAR_ZERO;
    	rC[9] = SCALAR_ZERO;
    	rC[10] = SCALAR_ZERO;
    	rC[11] = SCALAR_ZERO;
    	rC[12] = SCALAR_ZERO;
    	rC[13] = SCALAR_ZERO;
    	rC[14] = SCALAR_ZERO;
    	rC[15] = SCALAR_ZERO;
	
	for(i=0;i<size_k;i+=8)
	{
		//gloabl -> lds
		global_a0 = *(global_address_A + 0); //global read A
		global_a1 = *(global_address_A + 1);
		global_b0 = *(global_address_B + 0); //global read BETA
		global_b1 = *(global_address_B + 1);
		
		global_address_A += DEPTH * size_m;
		global_address_B += DEPTH * size_n;
		
		//write lds
		*(local_write_A + 0) = global_a0;
		*(local_write_A + 1) = global_a1;
		*(local_write_B + 0) = global_b0;
		*(local_write_B + 1) = global_b1;
		
		__syncthreads();
		
		//preload
		rA[0] = *(local_read_A + 0);
		rA[1] = *(local_read_A + 1);
		rA[2] = *(local_read_A + 2);
		rA[3] = *(local_read_A + 3);
			
		rB[0] = *(local_read_B + 0);
		rB[1] = *(local_read_B + 1);
		rB[2] = *(local_read_B + 2);
		rB[3] = *(local_read_B + 3);
	
		local_read_A += MT0I;
		local_read_B += MT1J;	
		
		//iter 0
		rA[0+TT] = *(local_read_A + 0);
		rA[1+TT] = *(local_read_A + 1);
		rA[2+TT] = *(local_read_A + 2);
		rA[3+TT] = *(local_read_A + 3);
			
		rB[0+TT] = *(local_read_B + 0);
		rB[1+TT] = *(local_read_B + 1);
		rB[2+TT] = *(local_read_B + 2);
		rB[3+TT] = *(local_read_B + 3);	
		
		local_read_A += MT0I;
		local_read_B += MT1J;
		MAC_4x4
		
		//iter 1
		rA[0] = *(local_read_A + 0);
		rA[1] = *(local_read_A + 1);
		rA[2] = *(local_read_A + 2);
		rA[3] = *(local_read_A + 3);
			
		rB[0] = *(local_read_B + 0);
		rB[1] = *(local_read_B + 1);
		rB[2] = *(local_read_B + 2);
		rB[3] = *(local_read_B + 3);
	
		local_read_A += MT0I;
		local_read_B += MT1J;	
		MAC_4x4_BLK	
		
		//iter 2
		rA[0+TT] = *(local_read_A + 0);
		rA[1+TT] = *(local_read_A + 1);
		rA[2+TT] = *(local_read_A + 2);
		rA[3+TT] = *(local_read_A + 3);
			
		rB[0+TT] = *(local_read_B + 0);
		rB[1+TT] = *(local_read_B + 1);
		rB[2+TT] = *(local_read_B + 2);
		rB[3+TT] = *(local_read_B + 3);	
		
		local_read_A += MT0I;
		local_read_B += MT1J;	
		MAC_4x4	
		
		//iter 3
		rA[0] = *(local_read_A + 0);
		rA[1] = *(local_read_A + 1);
		rA[2] = *(local_read_A + 2);
		rA[3] = *(local_read_A + 3);
			
		rB[0] = *(local_read_B + 0);
		rB[1] = *(local_read_B + 1);
		rB[2] = *(local_read_B + 2);
		rB[3] = *(local_read_B + 3);
		
		local_read_A += MT0I;
		local_read_B += MT1J;		
		MAC_4x4_BLK	
		
		//iter 4
		rA[0+TT] = *(local_read_A + 0);
		rA[1+TT] = *(local_read_A + 1);
		rA[2+TT] = *(local_read_A + 2);
		rA[3+TT] = *(local_read_A + 3);
			
		rB[0+TT] = *(local_read_B + 0);
		rB[1+TT] = *(local_read_B + 1);
		rB[2+TT] = *(local_read_B + 2);
		rB[3+TT] = *(local_read_B + 3);	
		
		local_read_A += MT0I;
		local_read_B += MT1J;	
		MAC_4x4	
		
		//iter 5
		rA[0] = *(local_read_A + 0);
		rA[1] = *(local_read_A + 1);
		rA[2] = *(local_read_A + 2);
		rA[3] = *(local_read_A + 3);
			
		rB[0] = *(local_read_B + 0);
		rB[1] = *(local_read_B + 1);
		rB[2] = *(local_read_B + 2);
		rB[3] = *(local_read_B + 3);
		
		local_read_A += MT0I;
		local_read_B += MT1J;
		MAC_4x4_BLK	
		
		//iter 6
		rA[0+TT] = *(local_read_A + 0);
		rA[1+TT] = *(local_read_A + 1);
		rA[2+TT] = *(local_read_A + 2);
		rA[3+TT] = *(local_read_A + 3);
			
		rB[0+TT] = *(local_read_B + 0);
		rB[1+TT] = *(local_read_B + 1);
		rB[2+TT] = *(local_read_B + 2);
		rB[3+TT] = *(local_read_B + 3);	
		
		MAC_4x4	
		
		local_read_A = localMemory + lri * 4;
		local_read_B = localMemory + lrj * 4 + LDS_OFFSET_B;
		
		MAC_4x4_BLK		

	}
	
	TYPE_MAC_WRITE(*(global_address_C+0), *(global_address_C+0), alpha, rC[0], beta);
	TYPE_MAC_WRITE(*(global_address_C+1), *(global_address_C+1), alpha, rC[1], beta);
	TYPE_MAC_WRITE(*(global_address_C+2), *(global_address_C+2), alpha, rC[2], beta);
	TYPE_MAC_WRITE(*(global_address_C+3), *(global_address_C+3), alpha, rC[3], beta);
	
	TYPE_MAC_WRITE(*(global_address_C+0+size_n), *(global_address_C+0+size_n), alpha, rC[4], beta);
	TYPE_MAC_WRITE(*(global_address_C+1+size_n), *(global_address_C+1+size_n), alpha, rC[5], beta);
	TYPE_MAC_WRITE(*(global_address_C+2+size_n), *(global_address_C+2+size_n), alpha, rC[6], beta);
	TYPE_MAC_WRITE(*(global_address_C+3+size_n), *(global_address_C+3+size_n), alpha, rC[7], beta);
	
	TYPE_MAC_WRITE(*(global_address_C+0+size_n*2), *(global_address_C+0+size_n*2), alpha, rC[8], beta);
	TYPE_MAC_WRITE(*(global_address_C+1+size_n*2), *(global_address_C+1+size_n*2), alpha, rC[9], beta);
	TYPE_MAC_WRITE(*(global_address_C+2+size_n*2), *(global_address_C+2+size_n*2), alpha, rC[10], beta);
	TYPE_MAC_WRITE(*(global_address_C+3+size_n*2), *(global_address_C+3+size_n*2), alpha, rC[11], beta);
	
	TYPE_MAC_WRITE(*(global_address_C+0+size_n*3), *(global_address_C+0+size_n*3), alpha, rC[12], beta);
	TYPE_MAC_WRITE(*(global_address_C+1+size_n*3), *(global_address_C+1+size_n*3), alpha, rC[13], beta);
	TYPE_MAC_WRITE(*(global_address_C+2+size_n*3), *(global_address_C+2+size_n*3), alpha, rC[14], beta);
	TYPE_MAC_WRITE(*(global_address_C+3+size_n*3), *(global_address_C+3+size_n*3), alpha, rC[15], beta);	
}
```

4x4划块，depth=8、数据预取并采用lds的 GEMM算法计算如图3.5.2.11所示。

<img src="https://s2.loli.net/2022/01/20/ko48DlVbPtiWRqg.png" alt="3_5_2_11_4x4划块_lds" style="zoom:80%;" />

​																图3.5.2.11 使用lds进行数据的搬移

下面我们对该代码进行分析。

- global memory->lds的数据读取

```C++
	/* allocate local memory */
    __shared__ double localMemory[LDS_NUM_ELEMENTS];
	
	unsigned int serial = threadIdx.x;  // 线程号
	unsigned int grj = (serial >> 5);   // global memory读取的行
	unsigned int gri = (serial & 31);   // global memory读取列的第一个元素
	
	unsigned int goa = grj * size_m + gri * 2;
	unsigned int gob = grj * size_n + gri * 2;
	
	double *local_write_A =  localMemory + serial * 2;
	double *local_write_B =  localMemory + serial * 2 + LDS_OFFSET_B;
...
	for(i=0;i<size_k;i+=8)
	{
		//gloabl -> lds
		global_a0 = *(global_address_A + 0); //global read A
		global_a1 = *(global_address_A + 1);
		global_b0 = *(global_address_B + 0); //global read B
		global_b1 = *(global_address_B + 1);
		
		global_address_A += DEPTH * size_m;
		global_address_B += DEPTH * size_n;
		
		//write lds
		*(local_write_A + 0) = global_a0;
		*(local_write_A + 1) = global_a1;
		*(local_write_B + 0) = global_b0;
		*(local_write_B + 1) = global_b1;
		
		__syncthreads();
        ...
    }
```

这段代码中，localMemory是定义的LDS。在该GEMM kernel的实现中，同样是用一个线程块，线程块仅x维度方向的大小为256，y、z维度均为1。grj与gri实际上是对256个线程进行重新映射，映射为8行（因为depth=8）,32列。每次读取A矩阵Mxdepth=64x8， B矩阵Nxdepth=64x8个数据。因此每个线程只需负责2个A矩阵数据，2个B矩阵数据，即可完成将global Memory中的本次depth=8区域内数据到LDS中的写入。

- lds数据到寄存器中的读取

```C++
	/* allocate local memory */
    __shared__ double localMemory[LDS_NUM_ELEMENTS];
	
	unsigned int serial = threadIdx.x;//线程号
	...
	unsigned int lrj = (serial >> 4);
	unsigned int lri = (serial & 15);
	
	double *local_read_A = localMemory + lri * 4;
	double *local_read_B = localMemory + lrj * 4 + LDS_OFFSET_B;
	...
	for(i=0;i<size_k;i+=8)
	{
		//gloabl -> lds
		global_a0 = *(global_address_A + 0); //global read A
		global_a1 = *(global_address_A + 1);
		global_b0 = *(global_address_B + 0); //global read BETA
		global_b1 = *(global_address_B + 1);
		
		global_address_A += DEPTH * size_m;
		global_address_B += DEPTH * size_n;
		
		//write lds
		*(local_write_A + 0) = global_a0;
		*(local_write_A + 1) = global_a1;
		*(local_write_B + 0) = global_b0;
		*(local_write_B + 1) = global_b1;
		
		__syncthreads();
		
		//preload
		rA[0] = *(local_read_A + 0);
		rA[1] = *(local_read_A + 1);
		rA[2] = *(local_read_A + 2);
		rA[3] = *(local_read_A + 3);
			
		rB[0] = *(local_read_B + 0);
		rB[1] = *(local_read_B + 1);
		rB[2] = *(local_read_B + 2);
		rB[3] = *(local_read_B + 3);
	
		local_read_A += MT0I;
		local_read_B += MT1J;	
		
		//iter 0
		rA[0+TT] = *(local_read_A + 0);
		rA[1+TT] = *(local_read_A + 1);
		rA[2+TT] = *(local_read_A + 2);
		rA[3+TT] = *(local_read_A + 3);
			
		rB[0+TT] = *(local_read_B + 0);
		rB[1+TT] = *(local_read_B + 1);
		rB[2+TT] = *(local_read_B + 2);
		rB[3+TT] = *(local_read_B + 3);	
     	...
    }
    
```

从lds中读取数据，与前几个小节从global memory中直接读取的逻辑是类似的，区别在于，原来列宽为size_m，size_n，现在变成了MT0I和MT1J，这里都是64。接下来的操作，从LDS中读取数据进行数据预取，和3.5.5节的计算流程类似，经测试该代码的浮点性能运算性能为14.02 GFlops。

​	由于LDS可以被线程块内所有线程访问以及LDS的访存速度远远高于global memory的访存速度的特性，我们可以使用LDS进行数据的搬移和访问，以减少访存的开销。通过理解该代码，我们可以看到，我们是将数据完全搬移到LDS中之后，才进行后续的读取到寄存器以及计算的流程，这种方式和3.5.4节介绍的一样，同样存在访存延迟的现象。

### 3.5.7 LDS交叉读取数据	

​	为了更好地掩藏延迟，我们可以将LDS的大小扩大一倍，一半用于进行读数据的操作，一半进行写数据的操作。修改后的代码如下所示：

```C++
#include <iostream>
#include <stdio.h>
#include <string.h>
#include "hip/hip_runtime.h"
#include <sys/time.h>

#define NUM 256

#define WARMUP

#define SCALAR_ZERO (double)(0)
#define LDS_NUM_ELEMENTS 2048
#define LDS_OFFSET_B 512
#define LDS_OFFSET_BLK 1024
#define DEPTH 8
#define MT0I 64
#define MT1J 64

/* MAC's */
#define MAC(A,B,DST) DST += A*B
#define TYPE_MAC(MULA,MULB,DST) DST = MAC(MULA,MULB,DST);
#define TT 4

/* 4x4 micro-tile */
#define MAC_4x4\
  TYPE_MAC(rA[0],rB[0],rC[0]); \
  TYPE_MAC(rA[0],rB[1],rC[1]); \
  TYPE_MAC(rA[0],rB[2],rC[2]); \
  TYPE_MAC(rA[0],rB[3],rC[3]); \
  TYPE_MAC(rA[1],rB[0],rC[4]); \
  TYPE_MAC(rA[1],rB[1],rC[5]); \
  TYPE_MAC(rA[1],rB[2],rC[6]); \
  TYPE_MAC(rA[1],rB[3],rC[7]); \
  TYPE_MAC(rA[2],rB[0],rC[8]); \
  TYPE_MAC(rA[2],rB[1],rC[9]); \
  TYPE_MAC(rA[2],rB[2],rC[10]); \
  TYPE_MAC(rA[2],rB[3],rC[11]); \
  TYPE_MAC(rA[3],rB[0],rC[12]); \
  TYPE_MAC(rA[3],rB[1],rC[13]); \
  TYPE_MAC(rA[3],rB[2],rC[14]); \
  TYPE_MAC(rA[3],rB[3],rC[15]); \
  
#define MAC_4x4_BLK\
  TYPE_MAC(rA[0+TT],rB[0+TT],rC[0]); \
  TYPE_MAC(rA[0+TT],rB[1+TT],rC[1]); \
  TYPE_MAC(rA[0+TT],rB[2+TT],rC[2]); \
  TYPE_MAC(rA[0+TT],rB[3+TT],rC[3]); \
  TYPE_MAC(rA[1+TT],rB[0+TT],rC[4]); \
  TYPE_MAC(rA[1+TT],rB[1+TT],rC[5]); \
  TYPE_MAC(rA[1+TT],rB[2+TT],rC[6]); \
  TYPE_MAC(rA[1+TT],rB[3+TT],rC[7]); \
  TYPE_MAC(rA[2+TT],rB[0+TT],rC[8]); \
  TYPE_MAC(rA[2+TT],rB[1+TT],rC[9]); \
  TYPE_MAC(rA[2+TT],rB[2+TT],rC[10]); \
  TYPE_MAC(rA[2+TT],rB[3+TT],rC[11]); \
  TYPE_MAC(rA[3+TT],rB[0+TT],rC[12]); \
  TYPE_MAC(rA[3+TT],rB[1+TT],rC[13]); \
  TYPE_MAC(rA[3+TT],rB[2+TT],rC[14]); \
  TYPE_MAC(rA[3+TT],rB[3+TT],rC[15]); \

#define TYPE_MAC_WRITE(DST,SRC,ALPHA,REG,BETA) DST = 0 != (BETA) ? (ALPHA)*(REG) + (BETA)*(SRC) : (ALPHA)*(REG)
__global__ void global_depth8_lds_2(double *src_a,double *src_b,double *dst_c, double alpha,double beta,int size_m,int size_n,int size_k)
{
	/* allocate local memory */
    __shared__ double localMemory[LDS_NUM_ELEMENTS];
	
	unsigned int serial = threadIdx.x;  // 1D线程号
	unsigned int grj = (serial >> 5);
	unsigned int gri = (serial & 31);
	
	unsigned int goa = grj * size_m + gri * 2;
	unsigned int gob = grj * size_n + gri * 2;
	
	unsigned int lwa = serial * 2;
	unsigned int lwb = serial * 2 + LDS_OFFSET_B;
	
	double *local_write_A =  localMemory + lwa;
	double *local_write_B =  localMemory + lwb;
	
	unsigned int lrj = (serial >> 4);
	unsigned int lri = (serial & 15);
	
	unsigned int lra = lri * 4;
	unsigned int lrb = lrj * 4 + LDS_OFFSET_B;
	
	double *local_read_A = localMemory + lra;
	double *local_read_B = localMemory + lrb;
	
	unsigned int goc = (lri * size_n + lrj ) * 4;
	
	double *global_address_C = dst_c + goc;
	double *global_address_A = src_a + goa;
	double *global_address_B = src_b + gob;
	
	int i,j,k;
	
	double rA[8],rB[8],rC[16];
	double global_a0,global_a1,global_b0,global_b1;
	
	rC[0] = SCALAR_ZERO;
        rC[1] = SCALAR_ZERO;
    	rC[2] = SCALAR_ZERO;
    	rC[3] = SCALAR_ZERO;
    	rC[4] = SCALAR_ZERO;
    	rC[5] = SCALAR_ZERO;
    	rC[6] = SCALAR_ZERO;
    	rC[7] = SCALAR_ZERO;
    	rC[8] = SCALAR_ZERO;
    	rC[9] = SCALAR_ZERO;
    	rC[10] = SCALAR_ZERO;
    	rC[11] = SCALAR_ZERO;
    	rC[12] = SCALAR_ZERO;
    	rC[13] = SCALAR_ZERO;
    	rC[14] = SCALAR_ZERO;
    	rC[15] = SCALAR_ZERO;
		
		//gloabl -> lds
		global_a0 = *(global_address_A + 0); //global read A
		global_a1 = *(global_address_A + 1);
		global_b0 = *(global_address_B + 0); //global read BETA
		global_b1 = *(global_address_B + 1);
		
		global_address_A += DEPTH * size_m;
		global_address_B += DEPTH * size_n;
		
		//write lds
		*(local_write_A + 0) = global_a0;
		*(local_write_A + 1) = global_a1;
		*(local_write_B + 0) = global_b0;
		*(local_write_B + 1) = global_b1;
		
		lwa = (lwa + LDS_OFFSET_BLK) % LDS_NUM_ELEMENTS;
		lwb = (lwb + LDS_OFFSET_BLK) % LDS_NUM_ELEMENTS;
		
		local_write_A =  localMemory + lwa;
		local_write_B =  localMemory + lwb;
		
		__syncthreads();
		
		//preload
		rA[0] = *(local_read_A + 0);
		rA[1] = *(local_read_A + 1);
		rA[2] = *(local_read_A + 2);
		rA[3] = *(local_read_A + 3);
			
		rB[0] = *(local_read_B + 0);
		rB[1] = *(local_read_B + 1);
		rB[2] = *(local_read_B + 2);
		rB[3] = *(local_read_B + 3);
	
		local_read_A += MT0I;
		local_read_B += MT1J;	
	
	for(i=0;i<size_k;i+=8)
	{	
		//gloabl -> lds
		global_a0 = *(global_address_A + 0); //global read A
		global_a1 = *(global_address_A + 1);
		global_b0 = *(global_address_B + 0); //global read BETA
		global_b1 = *(global_address_B + 1);
		
		global_address_A += DEPTH * size_m;
		global_address_B += DEPTH * size_n;
		
		//iter 0
		rA[0+TT] = *(local_read_A + 0);
		rA[1+TT] = *(local_read_A + 1);
		rA[2+TT] = *(local_read_A + 2);
		rA[3+TT] = *(local_read_A + 3);
			
		rB[0+TT] = *(local_read_B + 0);
		rB[1+TT] = *(local_read_B + 1);
		rB[2+TT] = *(local_read_B + 2);
		rB[3+TT] = *(local_read_B + 3);	
		
		local_read_A += MT0I;
		local_read_B += MT1J;
		MAC_4x4
		
		//iter 1
		rA[0] = *(local_read_A + 0);
		rA[1] = *(local_read_A + 1);
		rA[2] = *(local_read_A + 2);
		rA[3] = *(local_read_A + 3);
			
		rB[0] = *(local_read_B + 0);
		rB[1] = *(local_read_B + 1);
		rB[2] = *(local_read_B + 2);
		rB[3] = *(local_read_B + 3);
	
		local_read_A += MT0I;
		local_read_B += MT1J;	
		MAC_4x4_BLK	
		
		//iter 2
		rA[0+TT] = *(local_read_A + 0);
		rA[1+TT] = *(local_read_A + 1);
		rA[2+TT] = *(local_read_A + 2);
		rA[3+TT] = *(local_read_A + 3);
			
		rB[0+TT] = *(local_read_B + 0);
		rB[1+TT] = *(local_read_B + 1);
		rB[2+TT] = *(local_read_B + 2);
		rB[3+TT] = *(local_read_B + 3);	
		
		local_read_A += MT0I;
		local_read_B += MT1J;	
		MAC_4x4	
		
		//iter 3
		rA[0] = *(local_read_A + 0);
		rA[1] = *(local_read_A + 1);
		rA[2] = *(local_read_A + 2);
		rA[3] = *(local_read_A + 3);
			
		rB[0] = *(local_read_B + 0);
		rB[1] = *(local_read_B + 1);
		rB[2] = *(local_read_B + 2);
		rB[3] = *(local_read_B + 3);
		
		local_read_A += MT0I;
		local_read_B += MT1J;		
		MAC_4x4_BLK	
		
		//iter 4
		rA[0+TT] = *(local_read_A + 0);
		rA[1+TT] = *(local_read_A + 1);
		rA[2+TT] = *(local_read_A + 2);
		rA[3+TT] = *(local_read_A + 3);
			
		rB[0+TT] = *(local_read_B + 0);
		rB[1+TT] = *(local_read_B + 1);
		rB[2+TT] = *(local_read_B + 2);
		rB[3+TT] = *(local_read_B + 3);	
		
		local_read_A += MT0I;
		local_read_B += MT1J;	
		MAC_4x4	
		
		//iter 5
		rA[0] = *(local_read_A + 0);
		rA[1] = *(local_read_A + 1);
		rA[2] = *(local_read_A + 2);
		rA[3] = *(local_read_A + 3);
			
		rB[0] = *(local_read_B + 0);
		rB[1] = *(local_read_B + 1);
		rB[2] = *(local_read_B + 2);
		rB[3] = *(local_read_B + 3);
		
		local_read_A += MT0I;
		local_read_B += MT1J;
		MAC_4x4_BLK	
		
		//iter 6
		rA[0+TT] = *(local_read_A + 0);
		rA[1+TT] = *(local_read_A + 1);
		rA[2+TT] = *(local_read_A + 2);
		rA[3+TT] = *(local_read_A + 3);
			
		rB[0+TT] = *(local_read_B + 0);
		rB[1+TT] = *(local_read_B + 1);
		rB[2+TT] = *(local_read_B + 2);
		rB[3+TT] = *(local_read_B + 3);		
		
		//write lds
		*(local_write_A + 0) = global_a0;
		*(local_write_A + 1) = global_a1;
		*(local_write_B + 0) = global_b0;
		*(local_write_B + 1) = global_b1;
		
		lwa = ( lwa + LDS_OFFSET_BLK ) % LDS_NUM_ELEMENTS;
		lwb = ( lwb + LDS_OFFSET_BLK ) % LDS_NUM_ELEMENTS;
		
		local_write_A =  localMemory + lwa;
		local_write_B =  localMemory + lwb ;
		
		lra = (lra + LDS_OFFSET_BLK) % LDS_NUM_ELEMENTS;
		lrb = (lrb + LDS_OFFSET_BLK) % LDS_NUM_ELEMENTS;
		
		local_read_A = localMemory + lra;
		local_read_B = localMemory + lrb;
		
		MAC_4x4
		__syncthreads();
		
		//preload
		rA[0] = *(local_read_A + 0);
		rA[1] = *(local_read_A + 1);
		rA[2] = *(local_read_A + 2);
		rA[3] = *(local_read_A + 3);
			
		rB[0] = *(local_read_B + 0);
		rB[1] = *(local_read_B + 1);
		rB[2] = *(local_read_B + 2);
		rB[3] = *(local_read_B + 3);
	
		local_read_A += MT0I;
		local_read_B += MT1J;	
		
		MAC_4x4_BLK
	}
	
	TYPE_MAC_WRITE(*(global_address_C+0), *(global_address_C+0), alpha, rC[0], beta);
	TYPE_MAC_WRITE(*(global_address_C+1), *(global_address_C+1), alpha, rC[1], beta);
	TYPE_MAC_WRITE(*(global_address_C+2), *(global_address_C+2), alpha, rC[2], beta);
	TYPE_MAC_WRITE(*(global_address_C+3), *(global_address_C+3), alpha, rC[3], beta);
	
	TYPE_MAC_WRITE(*(global_address_C+0+size_n), *(global_address_C+0+size_n), alpha, rC[4], beta);
	TYPE_MAC_WRITE(*(global_address_C+1+size_n), *(global_address_C+1+size_n), alpha, rC[5], beta);
	TYPE_MAC_WRITE(*(global_address_C+2+size_n), *(global_address_C+2+size_n), alpha, rC[6], beta);
	TYPE_MAC_WRITE(*(global_address_C+3+size_n), *(global_address_C+3+size_n), alpha, rC[7], beta);
	
	TYPE_MAC_WRITE(*(global_address_C+0+size_n*2), *(global_address_C+0+size_n*2), alpha, rC[8], beta);
	TYPE_MAC_WRITE(*(global_address_C+1+size_n*2), *(global_address_C+1+size_n*2), alpha, rC[9], beta);
	TYPE_MAC_WRITE(*(global_address_C+2+size_n*2), *(global_address_C+2+size_n*2), alpha, rC[10], beta);
	TYPE_MAC_WRITE(*(global_address_C+3+size_n*2), *(global_address_C+3+size_n*2), alpha, rC[11], beta);
	
	TYPE_MAC_WRITE(*(global_address_C+0+size_n*3), *(global_address_C+0+size_n*3), alpha, rC[12], beta);
	TYPE_MAC_WRITE(*(global_address_C+1+size_n*3), *(global_address_C+1+size_n*3), alpha, rC[13], beta);
	TYPE_MAC_WRITE(*(global_address_C+2+size_n*3), *(global_address_C+2+size_n*3), alpha, rC[14], beta);
	TYPE_MAC_WRITE(*(global_address_C+3+size_n*3), *(global_address_C+3+size_n*3), alpha, rC[15], beta);
}
```

从代码中可以看出，为了掩藏延时，可以设置2块lds，这样一块进行读数据操作，一块进行写数据操作，如图3.5.2.12所示。

![](https://s2.loli.net/2022/01/20/pVOeW4So19aZCIr.png)

​																	图3.5.2.12 lds双buffer的使用

我们利用如下代码，进行读写buffer的切换，例如本次使用buffer1读，往buffer2中写；则下次使用buffer2读，往buffer1中写：

```C++
...	
lwa = ( lwa + LDS_OFFSET_BLK ) % LDS_NUM_ELEMENTS;
lwb = ( lwb + LDS_OFFSET_BLK ) % LDS_NUM_ELEMENTS;

local_write_A =  localMemory + lwa;
local_write_B =  localMemory + lwb ;

lra = (lra + LDS_OFFSET_BLK) % LDS_NUM_ELEMENTS;
lrb = (lrb + LDS_OFFSET_BLK) % LDS_NUM_ELEMENTS;

local_read_A = localMemory + lra;
local_read_B = localMemory + lrb;
...
```

代码中可以控制好global到lds的数据搬运时间,除了第一个是全部搬运完再进行计算，剩余循环的数据搬运可以和其他指令穿插进行，便于延时控制，数据的读取都是从LDS中直接读取的，LDS在线程之间公用，可以减少数据读取的耗时。其他代码逻辑，与3.5.6节介绍的类似。

​	在该代码中，采用两块LDS进行交替读取数据的方式，经测试该代码的浮点性能运算性能为15.07GFlops。在使用LDS进行性能优化时，我们还需要考虑到的一个问题是，减少bank冲突。

### 3.5.8 减少bank冲突

​	对上面的代码进行分析，可以知道：数据的读取是连续读取的，由于LDS中有32个bank，假设线程0在bank0上，按照这种读取方式会产生4路冲突。针对bank冲突的情况，我们继续进行优化，优化的代码如下：

```C++
#include <iostream>
#include <stdio.h>
#include <string.h>
#include "hip/hip_runtime.h"
#include <sys/time.h>

#define NUM 256
#define WARMUP

#define SCALAR_ZERO (double)(0)
#define LDS_NUM_ELEMENTS 2048
#define LDS_OFFSET_B 512
#define LDS_OFFSET_BLK 1024
#define DEPTH 8
#define MT0I 64
#define MT1J 64

/* MAC's */
#define MAC(A,B,DST) DST += A*B
#define TYPE_MAC(MULA,MULB,DST) DST = MAC(MULA,MULB,DST);
#define TT 4
#define SIZE_HALF_A 32
#define SIZE_HALF_B 32
#define SIZE_HALF_C 32

/* 4x4 micro-tile */
#define MAC_4x4\
  TYPE_MAC(rA[0],rB[0],rC[0]); \
  TYPE_MAC(rA[0],rB[1],rC[1]); \
  TYPE_MAC(rA[0],rB[2],rC[2]); \
  TYPE_MAC(rA[0],rB[3],rC[3]); \
  TYPE_MAC(rA[1],rB[0],rC[4]); \
  TYPE_MAC(rA[1],rB[1],rC[5]); \
  TYPE_MAC(rA[1],rB[2],rC[6]); \
  TYPE_MAC(rA[1],rB[3],rC[7]); \
  TYPE_MAC(rA[2],rB[0],rC[8]); \
  TYPE_MAC(rA[2],rB[1],rC[9]); \
  TYPE_MAC(rA[2],rB[2],rC[10]); \
  TYPE_MAC(rA[2],rB[3],rC[11]); \
  TYPE_MAC(rA[3],rB[0],rC[12]); \
  TYPE_MAC(rA[3],rB[1],rC[13]); \
  TYPE_MAC(rA[3],rB[2],rC[14]); \
  TYPE_MAC(rA[3],rB[3],rC[15]); \
  
#define MAC_4x4_BLK\
  TYPE_MAC(rA[0+TT],rB[0+TT],rC[0]); \
  TYPE_MAC(rA[0+TT],rB[1+TT],rC[1]); \
  TYPE_MAC(rA[0+TT],rB[2+TT],rC[2]); \
  TYPE_MAC(rA[0+TT],rB[3+TT],rC[3]); \
  TYPE_MAC(rA[1+TT],rB[0+TT],rC[4]); \
  TYPE_MAC(rA[1+TT],rB[1+TT],rC[5]); \
  TYPE_MAC(rA[1+TT],rB[2+TT],rC[6]); \
  TYPE_MAC(rA[1+TT],rB[3+TT],rC[7]); \
  TYPE_MAC(rA[2+TT],rB[0+TT],rC[8]); \
  TYPE_MAC(rA[2+TT],rB[1+TT],rC[9]); \
  TYPE_MAC(rA[2+TT],rB[2+TT],rC[10]); \
  TYPE_MAC(rA[2+TT],rB[3+TT],rC[11]); \
  TYPE_MAC(rA[3+TT],rB[0+TT],rC[12]); \
  TYPE_MAC(rA[3+TT],rB[1+TT],rC[13]); \
  TYPE_MAC(rA[3+TT],rB[2+TT],rC[14]); \
  TYPE_MAC(rA[3+TT],rB[3+TT],rC[15]); \

#define TYPE_MAC_WRITE(DST,SRC,ALPHA,REG,BETA) DST = 0 != (BETA) ? (ALPHA)*(REG) + (BETA)*(SRC) : (ALPHA)*(REG)

__global__ void global_depth8_lds_2_bank(double *src_a,double *src_b,double *dst_c, double alpha,double beta,int size_m,int size_n,int size_k)
{
    __shared__ double localMemory[LDS_NUM_ELEMENTS];
	
	unsigned int serial = threadIdx.x;//线程号
	unsigned int grj = (serial >> 5);
	unsigned int gri = (serial & 31);
	
	unsigned int goa = grj * size_m + gri * 2;
	unsigned int gob = grj * size_n + gri * 2;
	
	unsigned int lwa = serial * 2;
	unsigned int lwb = serial * 2 + LDS_OFFSET_B;
	
	double *local_write_A =  localMemory + lwa;
	double *local_write_B =  localMemory + lwb;
	
	unsigned int lrj = (serial >> 4);
	unsigned int lri = (serial & 15);
	
	unsigned int lra = lri * 2;
	unsigned int lrb = lrj * 2 + LDS_OFFSET_B;
	
	double *local_read_A = localMemory + lra;
	double *local_read_B = localMemory + lrb;
	
	unsigned int goc = (lri * size_n + lrj ) * 2;
	
	double *global_address_C = dst_c + goc;
	double *global_address_A = src_a + goa;
	double *global_address_B = src_b + gob;
	
	int i,j,k;
	
	double rA[8],rB[8],rC[16];
	double global_a0,global_a1,global_b0,global_b1;
	
	rC[0] = SCALAR_ZERO;
        rC[1] = SCALAR_ZERO;
    	rC[2] = SCALAR_ZERO;
    	rC[3] = SCALAR_ZERO;
    	rC[4] = SCALAR_ZERO;
    	rC[5] = SCALAR_ZERO;
    	rC[6] = SCALAR_ZERO;
    	rC[7] = SCALAR_ZERO;
    	rC[8] = SCALAR_ZERO;
    	rC[9] = SCALAR_ZERO;
    	rC[10] = SCALAR_ZERO;
    	rC[11] = SCALAR_ZERO;
    	rC[12] = SCALAR_ZERO;
    	rC[13] = SCALAR_ZERO;
    	rC[14] = SCALAR_ZERO;
    	rC[15] = SCALAR_ZERO;
		
		//gloabl -> lds
		global_a0 = *(global_address_A + 0); //global read A
		global_a1 = *(global_address_A + 1);
		global_b0 = *(global_address_B + 0); //global read BETA
		global_b1 = *(global_address_B + 1);
		
		global_address_A += DEPTH * size_m;
		global_address_B += DEPTH * size_n;
		
		//write lds
		*(local_write_A + 0) = global_a0;
		*(local_write_A + 1) = global_a1;
		*(local_write_B + 0) = global_b0;
		*(local_write_B + 1) = global_b1;
		
		lwa = (lwa + LDS_OFFSET_BLK) % LDS_NUM_ELEMENTS;
		lwb = (lwb + LDS_OFFSET_BLK) % LDS_NUM_ELEMENTS;
		
		local_write_A =  localMemory + lwa;
		local_write_B =  localMemory + lwb;
		
		__syncthreads();
		
		//preload
		rA[0] = *(local_read_A + 0);
		rA[1] = *(local_read_A + 1);
		rA[2] = *(local_read_A + 0 + SIZE_HALF_A);
		rA[3] = *(local_read_A + 1 + SIZE_HALF_A);
			
		rB[0] = *(local_read_B + 0);
		rB[1] = *(local_read_B + 1);
		rB[2] = *(local_read_B + 0 + SIZE_HALF_B);
		rB[3] = *(local_read_B + 1 + SIZE_HALF_B);
	
		local_read_A += MT0I;
		local_read_B += MT1J;	
	
	for(i=0;i<size_k;i+=8)
	{	
		//gloabl -> lds
		global_a0 = *(global_address_A + 0); //global read A
		global_a1 = *(global_address_A + 1);
		global_b0 = *(global_address_B + 0); //global read BETA
		global_b1 = *(global_address_B + 1);
		
		global_address_A += DEPTH * size_m;
		global_address_B += DEPTH * size_n;
		
		//iter 0
		rA[0+TT] = *(local_read_A + 0);
		rA[1+TT] = *(local_read_A + 1);
		rA[2+TT] = *(local_read_A + 0 + SIZE_HALF_A);
		rA[3+TT] = *(local_read_A + 1 + SIZE_HALF_A);
		
		rB[0+TT] = *(local_read_B + 0);
		rB[1+TT] = *(local_read_B + 1);
		rB[2+TT] = *(local_read_B + 0 + SIZE_HALF_B);
		rB[3+TT] = *(local_read_B + 1 + SIZE_HALF_B);	
		
		local_read_A += MT0I;
		local_read_B += MT1J;
		MAC_4x4
		
		//iter 1
		rA[0] = *(local_read_A + 0);
		rA[1] = *(local_read_A + 1);
		rA[2] = *(local_read_A + 0 + SIZE_HALF_A);
		rA[3] = *(local_read_A + 1 + SIZE_HALF_A);
			
		rB[0] = *(local_read_B + 0);
		rB[1] = *(local_read_B + 1);
		rB[2] = *(local_read_B + 0 + SIZE_HALF_B);
		rB[3] = *(local_read_B + 1 + SIZE_HALF_B);
	
		local_read_A += MT0I;
		local_read_B += MT1J;	
		MAC_4x4_BLK	
		
		//iter 2
		rA[0+TT] = *(local_read_A + 0);
		rA[1+TT] = *(local_read_A + 1);
		rA[2+TT] = *(local_read_A + 0 + SIZE_HALF_A);
		rA[3+TT] = *(local_read_A + 1 + SIZE_HALF_A);
			
		rB[0+TT] = *(local_read_B + 0);
		rB[1+TT] = *(local_read_B + 1);
		rB[2+TT] = *(local_read_B + 0 + SIZE_HALF_B);
		rB[3+TT] = *(local_read_B + 1 + SIZE_HALF_B);	
		
		local_read_A += MT0I;
		local_read_B += MT1J;	
		MAC_4x4	
		
		//iter 3
		rA[0] = *(local_read_A + 0);
		rA[1] = *(local_read_A + 1);
		rA[2] = *(local_read_A + 0 + SIZE_HALF_A);
		rA[3] = *(local_read_A + 1 + SIZE_HALF_A);
			
		rB[0] = *(local_read_B + 0);
		rB[1] = *(local_read_B + 1);
		rB[2] = *(local_read_B + 0 + SIZE_HALF_B);
		rB[3] = *(local_read_B + 1 + SIZE_HALF_B);
		
		local_read_A += MT0I;
		local_read_B += MT1J;		
		MAC_4x4_BLK	
		
		//iter 4
		rA[0+TT] = *(local_read_A + 0);
		rA[1+TT] = *(local_read_A + 1);
		rA[2+TT] = *(local_read_A + 0 + SIZE_HALF_A);
		rA[3+TT] = *(local_read_A + 1 + SIZE_HALF_A);
			
		rB[0+TT] = *(local_read_B + 0);
		rB[1+TT] = *(local_read_B + 1);
		rB[2+TT] = *(local_read_B + 0 + SIZE_HALF_B);
		rB[3+TT] = *(local_read_B + 1 + SIZE_HALF_B);	
		
		local_read_A += MT0I;
		local_read_B += MT1J;	
		MAC_4x4	
		
		//iter 5
		rA[0] = *(local_read_A + 0);
		rA[1] = *(local_read_A + 1);
		rA[2] = *(local_read_A + 0 + SIZE_HALF_A);
		rA[3] = *(local_read_A + 1 + SIZE_HALF_A);
			
		rB[0] = *(local_read_B + 0);
		rB[1] = *(local_read_B + 1);
		rB[2] = *(local_read_B + 0 + SIZE_HALF_B);
		rB[3] = *(local_read_B + 1 + SIZE_HALF_B);
		
		local_read_A += MT0I;
		local_read_B += MT1J;
		MAC_4x4_BLK	
		
		//iter 6
		rA[0+TT] = *(local_read_A + 0);
		rA[1+TT] = *(local_read_A + 1);
		rA[2+TT] = *(local_read_A + 0 + SIZE_HALF_A);
		rA[3+TT] = *(local_read_A + 1 + SIZE_HALF_A);
			
		rB[0+TT] = *(local_read_B + 0);
		rB[1+TT] = *(local_read_B + 1);
		rB[2+TT] = *(local_read_B + 0 + SIZE_HALF_B);
		rB[3+TT] = *(local_read_B + 1 + SIZE_HALF_B);		
		
		//write lds
		*(local_write_A + 0) = global_a0;
		*(local_write_A + 1) = global_a1;
		*(local_write_B + 0) = global_b0;
		*(local_write_B + 1) = global_b1;
		
		lwa = ( lwa + LDS_OFFSET_BLK ) % LDS_NUM_ELEMENTS;
		lwb = ( lwb + LDS_OFFSET_BLK ) % LDS_NUM_ELEMENTS;
		
		local_write_A =  localMemory + lwa;
		local_write_B =  localMemory + lwb ;
		
		lra = (lra + LDS_OFFSET_BLK) % LDS_NUM_ELEMENTS;
		lrb = (lrb + LDS_OFFSET_BLK) % LDS_NUM_ELEMENTS;
		
		local_read_A = localMemory + lra;
		local_read_B = localMemory + lrb;
		
		MAC_4x4
		__syncthreads();
		
		//preload
		rA[0] = *(local_read_A + 0);
		rA[1] = *(local_read_A + 1);
		rA[2] = *(local_read_A + 0 + SIZE_HALF_A);
		rA[3] = *(local_read_A + 1 + SIZE_HALF_A);
			
		rB[0] = *(local_read_B + 0);
		rB[1] = *(local_read_B + 1);
		rB[2] = *(local_read_B + 0 + SIZE_HALF_B);
		rB[3] = *(local_read_B + 1 + SIZE_HALF_B);
	
		local_read_A += MT0I;
		local_read_B += MT1J;	
		
		MAC_4x4_BLK
	}
	
	TYPE_MAC_WRITE(*(global_address_C+0), *(global_address_C+0), alpha, rC[0], beta);
	TYPE_MAC_WRITE(*(global_address_C+1), *(global_address_C+1), alpha, rC[1], beta);
	TYPE_MAC_WRITE(*(global_address_C+0+SIZE_HALF_C), *(global_address_C+0+SIZE_HALF_C), alpha, rC[2], beta);
	TYPE_MAC_WRITE(*(global_address_C+1+SIZE_HALF_C), *(global_address_C+1+SIZE_HALF_C), alpha, rC[3], beta);
	
	TYPE_MAC_WRITE(*(global_address_C+0+size_n), *(global_address_C+0+size_n), alpha, rC[4], beta);
	TYPE_MAC_WRITE(*(global_address_C+1+size_n), *(global_address_C+1+size_n), alpha, rC[5], beta);
	TYPE_MAC_WRITE(*(global_address_C+0+SIZE_HALF_C+size_n), *(global_address_C+0+SIZE_HALF_C+size_n), alpha, rC[6], beta);
	TYPE_MAC_WRITE(*(global_address_C+1+SIZE_HALF_C+size_n), *(global_address_C+1+SIZE_HALF_C+size_n), alpha, rC[7], beta);
	
	TYPE_MAC_WRITE(*(global_address_C+0+size_n*SIZE_HALF_C), *(global_address_C+0+size_n*SIZE_HALF_C), alpha, rC[8], beta);
	TYPE_MAC_WRITE(*(global_address_C+1+size_n*SIZE_HALF_C), *(global_address_C+1+size_n*SIZE_HALF_C), alpha, rC[9], beta);
	TYPE_MAC_WRITE(*(global_address_C+0+SIZE_HALF_C+size_n*SIZE_HALF_C), *(global_address_C+0+SIZE_HALF_C+size_n*SIZE_HALF_C), alpha, rC[10], beta);
	TYPE_MAC_WRITE(*(global_address_C+1+SIZE_HALF_C+size_n*SIZE_HALF_C), *(global_address_C+1+SIZE_HALF_C+size_n*SIZE_HALF_C), alpha, rC[11], beta);
	
	TYPE_MAC_WRITE(*(global_address_C+0+size_n*(SIZE_HALF_C+1)), *(global_address_C+0+size_n*(SIZE_HALF_C+1)), alpha, rC[12], beta);
	TYPE_MAC_WRITE(*(global_address_C+1+size_n*(SIZE_HALF_C+1)), *(global_address_C+1+size_n*(SIZE_HALF_C+1)), alpha, rC[13], beta);
	TYPE_MAC_WRITE(*(global_address_C+0+SIZE_HALF_C+size_n*(SIZE_HALF_C+1)), *(global_address_C+0+SIZE_HALF_C+size_n*(SIZE_HALF_C+1)), alpha, rC[14], beta);
	TYPE_MAC_WRITE(*(global_address_C+1+SIZE_HALF_C+size_n*(SIZE_HALF_C+1)), *(global_address_C+1+SIZE_HALF_C+size_n*(SIZE_HALF_C+1)), alpha, rC[15], beta);
	
}
```

​	下面我们首先对3.5.7节lds访存中存在的bank冲突的情况进行分析。在该代码中，我们使用1个线程块，并且该线程块只有x维度不为1，线程块中有256个线程。每个线程，映射到对应的4x4的小矩阵块：

```C++
unsigned int serial  = threadIdx.x;  // 1D 线程号
unsigned int lrj = (serial >> 4);    // 当前小矩阵块左上角元素所在的C矩阵的列
unsigned int lri = (serial & 15);    // 当前小矩阵块左上角元素所在的C矩阵的行
...
unsigned int lra = lri * 4;
unsigned int lrb = lrj * 4 + LDS_OFFSET_B;

double *local_read_A = localMemory + lra;
double *local_read_B = localMemory + lrb;
...
rA[0] = *(local_read_A + 0);
rA[1] = *(local_read_A + 1);
rA[2] = *(local_read_A + 2);
rA[3] = *(local_read_A + 3);

rB[0] = *(local_read_B + 0);
rB[1] = *(local_read_B + 1);
rB[2] = *(local_read_B + 2);
rB[3] = *(local_read_B + 3);
```

在一个wavefront 64个线程内部，线程对应的4x4小块的行列索引为：

thread0:(0,0)               thread16:(0,1)       thread32:(0,2)    thread48:(0,3)

thread1:(1,0)               thread17:(1,1)       thread33:(1,2)    thread49:(1,3)

thread2:(2,0)               thread18:(2,1)       thread34:(2,2)    thread50:(3,3)

...

thread15:(15, 0)         thread31:(15,1)     thread47:(15,2)   thread63:(3,3)

则：

thread0连续访问A矩阵4个数据，占用bank0-bank7, 连续访问B矩阵4个数据，占用bank0-bank7；

thread1连续访问A矩阵4个数据，占用bank8-bank15, 连续访问B矩阵4个数据，占用bank0-bank7；

thread2连续访问A矩阵4个数据，占用bank16-bank23, 连续访问B矩阵4个数据，占用bank0-bank7；

thread3连续访问A矩阵4个数据，占用bank24-bank31, 连续访问B矩阵4个数据，占用bank0-bank7；

thread4连续访问A矩阵4个数据，占用bank0-bank7, 连续访问B矩阵4个数据，占用bank0-bank7；

thread5连续访问A矩阵4个数据，占用bank8-bank15, 连续访问B矩阵4个数据，占用bank0-bank7；

thread6连续访问A矩阵4个数据，占用bank16-bank23, 连续访问B矩阵4个数据，占用bank0-bank7；

thread7连续访问A矩阵4个数据，占用bank24-bank31, 连续访问B矩阵4个数据，占用bank0-bank7；

thread8连续访问A矩阵4个数据，占用bank0-bank7, 连续访问B矩阵4个数据，占用bank0-bank7；

thread9连续访问A矩阵4个数据，占用bank8-bank15, 连续访问B矩阵4个数据，占用bank0-bank7；

thread10连续访问A矩阵4个数据，占用bank16-bank23, 连续访问B矩阵4个数据，占用bank0-bank7；

thread11连续访问A矩阵4个数据，占用bank24-bank31, 连续访问B矩阵4个数据，占用bank0-bank7；

thread12连续访问A矩阵4个数据，占用bank0-bank7, 连续访问B矩阵4个数据，占用bank0-bank7；

thread13连续访问A矩阵4个数据，占用bank8-bank15, 连续访问B矩阵4个数据，占用bank0-bank7；

thread14连续访问A矩阵4个数据，占用bank16-bank23, 连续访问B矩阵4个数据，占用bank0-bank7；

thread15连续访问A矩阵4个数据，占用bank24-bank31, 连续访问B矩阵4个数据，占用bank0-bank7；

...

thread48连续访问A矩阵4个数据，占用bank0-bank7, 连续访问B矩阵4个数据，占用bank24-bank31；

thread49连续访问A矩阵4个数据，占用bank8-bank15, 连续访问B矩阵4个数据，占用bank24-bank31；

thread50连续访问A矩阵4个数据，占用bank16-bank23, 连续访问B矩阵4个数据，占用bank24-bank31；

thread51连续访问A矩阵4个数据，占用bank24-bank31, 连续访问B矩阵4个数据，占用bank24-bank31；

thread52连续访问A矩阵4个数据，占用bank0-bank7, 连续访问B矩阵4个数据，占用bank24-bank31；

thread53连续访问A矩阵4个数据，占用bank8-bank15, 连续访问B矩阵4个数据，占用bank24-bank31；

thread54连续访问A矩阵4个数据，占用bank16-bank23, 连续访问B矩阵4个数据，占用bank24-bank31；

thread55连续访问A矩阵4个数据，占用bank24-bank31, 连续访问B矩阵4个数据，占用bank24-bank31；

thread56连续访问A矩阵4个数据，占用bank0-bank7, 连续访问B矩阵4个数据，占用bank24-bank31；

thread57连续访问A矩阵4个数据，占用bank8-bank15, 连续访问B矩阵4个数据，占用bank24-bank31；

thread58连续访问A矩阵4个数据，占用bank16-bank23, 连续访问B矩阵4个数据，占用bank24-bank31；

thread59连续访问A矩阵4个数据，占用bank24-bank31, 连续访问B矩阵4个数据，占用bank24-bank31；

thread60连续访问A矩阵4个数据，占用bank0-bank7, 连续访问B矩阵4个数据，占用bank24-bank31；

thread61连续访问A矩阵4个数据，占用bank8-bank15, 连续访问B矩阵4个数据，占用bank24-bank31；

thread62连续访问A矩阵4个数据，占用bank16-bank23, 连续访问B矩阵4个数据，占用bank24-bank31；

thread63连续访问A矩阵4个数据，占用bank24-bank31, 连续访问B矩阵4个数据，占用bank24-bank31；

从该分析可以看出，一个wavefront内部，对lds中A的访问存在4路bank冲突，对B的访问不存在bank冲突。

为了减少bank冲突，我们改变从LDS中进行数据读取的方式，如下：

```C++
//iter 0
#define SIZE_HALF_A 32
#define SIZE_HALF_B 32

...
rA[0+TT] = *(local_read_A + 0);
rA[1+TT] = *(local_read_A + 1);
rA[2+TT] = *(local_read_A + 0 + SIZE_HALF_A);
rA[3+TT] = *(local_read_A + 1 + SIZE_HALF_A);

rB[0+TT] = *(local_read_B + 0);
rB[1+TT] = *(local_read_B + 1);
rB[2+TT] = *(local_read_B + 0 + SIZE_HALF_B);
rB[3+TT] = *(local_read_B + 1 + SIZE_HALF_B);	
```

​	改写之后，我们进行分析如下：

thread0访问A矩阵4个数据，占用bank0-bank3, 访问B矩阵4个数据，占用bank0-bank3；

thread1访问A矩阵4个数据，占用bank4-bank7, 访问B矩阵4个数据，占用bank0-bank3；

thread2访问A矩阵4个数据，占用bank8-bank11, 访问B矩阵4个数据，占用bank0-bank3；

thread3访问A矩阵4个数据，占用bank12-bank15, 访问B矩阵4个数据，占用bank0-bank3；

thread4访问A矩阵4个数据，占用bank16-bank19, 访问B矩阵4个数据，占用bank0-bank3；

thread5访问A矩阵4个数据，占用bank20-bank23, 访问B矩阵4个数据，占用bank0-bank3；

thread6访问A矩阵4个数据，占用bank24-bank27, 访问B矩阵4个数据，占用bank0-bank3；

thread7访问A矩阵4个数据，占用bank28-bank31, 访问B矩阵4个数据，占用bank0-bank3；

thread8访问A矩阵4个数据，占用bank0-bank3, 访问B矩阵4个数据，占用bank0-bank3；

thread9访问A矩阵4个数据，占用bank4-bank7, 访问B矩阵4个数据，占用bank0-bank3；

thread10访问A矩阵4个数据，占用bank8-bank11, 访问B矩阵4个数据，占用bank0-bank3；

thread11访问A矩阵4个数据，占用bank12-bank15, 访问B矩阵4个数据，占用bank0-bank3；

thread12访问A矩阵4个数据，占用bank16-bank19, 访问B矩阵4个数据，占用bank0-bank3；

thread13访问A矩阵4个数据，占用bank20-bank23, 访问B矩阵4个数据，占用bank0-bank3；

thread14访问A矩阵4个数据，占用bank24-bank27, 访问B矩阵4个数据，占用bank0-bank3；

thread15访问A矩阵4个数据，占用bank28-bank31, 访问B矩阵4个数据，占用bank0-bank3；

...

thread48访问A矩阵4个数据，占用bank0-bank3, 访问B矩阵4个数据，占用bank12-bank15；

thread49访问A矩阵4个数据，占用bank4-bank7, 访问B矩阵4个数据，占用bank12-bank15；

thread50访问A矩阵4个数据，占用bank8-bank11, 访问B矩阵4个数据，占用bank12-bank15；

thread51访问A矩阵4个数据，占用bank12-bank15, 访问B矩阵4个数据，占用bank12-bank15；

thread52访问A矩阵4个数据，占用bank16-bank19, 访问B矩阵4个数据，占用bank12-bank15；

thread53访问A矩阵4个数据，占用bank20-bank23, 访问B矩阵4个数据，占用bank12-bank15；

thread54访问A矩阵4个数据，占用bank24-bank27, 访问B矩阵4个数据，占用bank12-bank15；

thread55访问A矩阵4个数据，占用bank28-bank31, 访问B矩阵4个数据，占用bank12-bank15；

thread56访问A矩阵4个数据，占用bank0-bank3, 访问B矩阵4个数据，占用bank12-bank15；

thread57访问A矩阵4个数据，占用bank4-bank7, 访问B矩阵4个数据，占用bank12-bank15；

thread58访问A矩阵4个数据，占用bank8-bank11, 访问B矩阵4个数据，占用bank12-bank15；

thread59访问A矩阵4个数据，占用bank12-bank15, 访问B矩阵4个数据，占用bank12-bank15；

thread60访问A矩阵4个数据，占用bank16-bank19, 访问B矩阵4个数据，占用bank12-bank15；

thread61访问A矩阵4个数据，占用bank20-bank23, 访问B矩阵4个数据，占用bank12-bank15；

thread62访问A矩阵4个数据，占用bank24-bank27, 访问B矩阵4个数据，占用bank12-bank15；

thread63访问A矩阵4个数据，占用bank28-bank31, 访问B矩阵4个数据，占用bank12-bank15；

​	通过这种数据读取的方式，我们可以将bank冲突从4路减少到2路，经测试该代码的浮点性能运算性能为16.62 GFlops。bank冲突是我们进行深度优化时，一个比较常见的优化点。

### 3.5.9 小结

​	本节以GEMM算法为例，将该算法移植到DCU上，并使用寄存器实现数据复用、利用数据预取来隐藏延迟以及使用LDS进行数据的搬移及数据的高效的访存，以及对LDS的bank冲突进行介绍。希望开发者，能够根据自己的应用采用合适的优化策略，在DCU上达到预期的性能提升。













